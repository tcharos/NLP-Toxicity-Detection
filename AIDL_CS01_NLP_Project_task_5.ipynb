{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "225227e1",
      "metadata": {
        "id": "225227e1"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tcharos/NLP-Toxicity-Detection/blob/main/AIDL_CS01_NLP_Project_task_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rekBaK9gR16r",
      "metadata": {
        "id": "rekBaK9gR16r"
      },
      "source": [
        "# AIDL_B_CS01: Advanced NLP Project\n",
        "\n",
        "## LLM Tuning with DPO (Gordon Ramsay Alignment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc91b44f",
      "metadata": {
        "id": "dc91b44f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import sys\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "BASE_DIR = \"/content\" if IN_COLAB else \".\"\n",
        "TOXICITY_PATH = os.path.join(BASE_DIR, \"data_sets/toxicity\")\n",
        "SEED = 12345\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"Running in Google Colab. Installing NLP stack...\")\n",
        "    !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "    !pip install -q -U \"trl<=0.24.0\" \"datasets==4.3.0\" transformers accelerate peft sentence-transformers\n",
        "else:\n",
        "    print(\"Running locally. Checking Mac-specific requirements...\")\n",
        "    !{sys.executable} -m pip install -q \"tensorflow==2.16.2\" \"tensorflow-macos==2.16.2\" \"tf-keras~=2.16\"\n",
        "    !{sys.executable} -m pip install unsloth-mlx\n",
        "    !{sys.executable} -m pip install -q -U \"trl<=0.24.0\" \"datasets==4.3.0\" transformers accelerate peft sentence-transformers\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "if IN_COLAB:\n",
        "    from unsloth import FastLanguageModel\n",
        "else:\n",
        "    from unsloth_mlx import FastLanguageModel\n",
        "\n",
        "from unsloth import PatchDPOTrainer\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "from datasets import Dataset\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "from datasets import load_dataset, Dataset\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import DPOConfig, DPOTrainer\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "print(f\"\\nTensorFlow Version: {tf.__version__}\")\n",
        "print(\"Num GPUs Available (TF): \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "HAS_MPS = torch.backends.mps.is_available()\n",
        "HAS_CUDA = torch.cuda.is_available()\n",
        "\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print(\"PyTorch Device: Mac GPU (Metal)\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"PyTorch Device: Colab GPU (CUDA)\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"PyTorch Device: CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e548f0da",
      "metadata": {
        "id": "e548f0da"
      },
      "source": [
        "### Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YDNTTte-ALFH",
      "metadata": {
        "id": "YDNTTte-ALFH"
      },
      "outputs": [],
      "source": [
        "# code i used to contatenate all *.csv to one test.csv - executed only once\n",
        "\n",
        "# all_csv_files = glob.glob(os.path.join(test_folder_path, \"*.csv\"))\n",
        "# valid_dfs = []\n",
        "# required_cols = [\"Question\", \"Polite\", \"Ramsay\"]\n",
        "\n",
        "# for f in all_csv_files:\n",
        "#     try:\n",
        "#         # Try UTF-8 first, fallback to cp1252 if it fails\n",
        "#         try:\n",
        "#             temp_df = pd.read_csv(f, encoding='utf-8')\n",
        "#         except UnicodeDecodeError:\n",
        "#             temp_df = pd.read_csv(f, encoding='cp1252')\n",
        "\n",
        "#         # Check if the required columns exist\n",
        "#         if all(col in temp_df.columns for col in required_cols):\n",
        "#             valid_dfs.append(temp_df[required_cols])\n",
        "#         else:\n",
        "#             print(f\"Skipping {f}: Missing required columns. Found: {temp_df.columns.tolist()}\")\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"Could not load {f} due to error: {e}\")\n",
        "\n",
        "# # Combine only the valid ones\n",
        "# if valid_dfs:\n",
        "#     all_colleagues_data = pd.concat(valid_dfs, ignore_index=True)\n",
        "#     # Requirement 4: Save to test.csv\n",
        "#     all_colleagues_data.to_csv(\"test.csv\", index=False)\n",
        "\n",
        "#     # Take 500 for training\n",
        "#     train_df = all_colleagues_data.sample(n=min(500, len(all_colleagues_data)), random_state=42)\n",
        "#     print(f\"Successfully loaded {len(valid_dfs)} files.\")\n",
        "#     print(f\"Total training rows available: {len(all_colleagues_data)}\")\n",
        "# else:\n",
        "#     print(\"No valid CSV files were loaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "814025e8",
      "metadata": {
        "id": "814025e8"
      },
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c071abdb",
      "metadata": {
        "id": "c071abdb"
      },
      "outputs": [],
      "source": [
        "def verify_and_get_files(folder, expected_default_name):\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "    # Check if any CSV already exists in the folder\n",
        "    existing_csvs = glob.glob(os.path.join(folder, \"*.csv\"))\n",
        "\n",
        "    if IN_COLAB and not existing_csvs:\n",
        "        print(f\"No CSV found in {folder}. Upload your file.\")\n",
        "        uploaded = files.upload()\n",
        "        for filename in uploaded.keys():\n",
        "            target_path = os.path.join(folder, filename)\n",
        "            os.rename(filename, target_path)\n",
        "        existing_csvs = glob.glob(os.path.join(folder, \"*.csv\"))\n",
        "\n",
        "    if existing_csvs:\n",
        "        for f in existing_csvs:\n",
        "            if os.path.basename(f) == expected_default_name:\n",
        "                return f\n",
        "        return existing_csvs[0]\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rH8ffhfPATtE",
      "metadata": {
        "id": "rH8ffhfPATtE"
      },
      "outputs": [],
      "source": [
        "test_folder_path = './data_sets/Ramsay/test'\n",
        "val_folder_path = './data_sets/Ramsay/val'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fc76ac2",
      "metadata": {
        "id": "0fc76ac2"
      },
      "outputs": [],
      "source": [
        "train_file = verify_and_get_files(test_folder_path, \"test.csv\")\n",
        "val_file = verify_and_get_files(val_folder_path, \"mscaidl-0077_ramsay_dataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9vqYKIkoHh-E",
      "metadata": {
        "id": "9vqYKIkoHh-E"
      },
      "outputs": [],
      "source": [
        "if train_file and val_file:\n",
        "    print(f\"Training file located: {train_file}\")\n",
        "    print(f\"Validation file located: {val_file}\")\n",
        "\n",
        "    try:\n",
        "        sample_df = pd.read_csv(val_file, sep=None, engine='python', encoding='utf-8-sig')\n",
        "        print(\"\\nSuccessfully loaded data. Preview of columns:\")\n",
        "        print(sample_df.columns.tolist())\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file: {e}\")\n",
        "else:\n",
        "    print(\"Files are missing. If you are not in Colab, please place CSVs in the folders manually.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Kpz4i4r6GPK2",
      "metadata": {
        "id": "Kpz4i4r6GPK2"
      },
      "outputs": [],
      "source": [
        "def load_any_ramsay_csv(file_path, limit=None, is_train=True):\n",
        "    with open(file_path, 'r', encoding='utf-8-sig', errors='ignore') as f:\n",
        "        content = f.read().replace('\"', '')\n",
        "\n",
        "    df = pd.read_csv(io.StringIO(content), sep=None, engine='python', on_bad_lines='skip')\n",
        "\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "\n",
        "    required_cols = [\"Question\", \"Polite\", \"Ramsay\"]\n",
        "    df = df[required_cols]\n",
        "\n",
        "    if is_train:\n",
        "        # 500 samples for training\n",
        "        df = df.sample(n=min(limit, len(df)), random_state=SEED)\n",
        "    else:\n",
        "        # first 100 samples for validation\n",
        "        df = df.head(limit)\n",
        "\n",
        "    print(f\"Successfully loaded {len(df)} rows from {file_path}\")\n",
        "\n",
        "    return Dataset.from_dict({\n",
        "        \"prompt\":   df[\"Question\"].astype(str).tolist(),\n",
        "        \"chosen\":   df[\"Polite\"].astype(str).tolist(),\n",
        "        \"rejected\": df[\"Ramsay\"].astype(str).tolist(),\n",
        "    }), df\n",
        "\n",
        "# train dataset\n",
        "train_dataset, _ = load_any_ramsay_csv(train_file, limit=500, is_train=True)\n",
        "\n",
        "# val dataset\n",
        "eval_dataset, eval_df_raw = load_any_ramsay_csv(val_file, limit=100, is_train=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NIKkbtJmR16w",
      "metadata": {
        "id": "NIKkbtJmR16w"
      },
      "source": [
        "### SLM from usloath (not Zephyr)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g7Vu9BRfR16w",
      "metadata": {
        "id": "g7Vu9BRfR16w"
      },
      "outputs": [],
      "source": [
        "model_name = \"unsloth/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "max_seq_length = 2048\n",
        "dtype = None # Auto detect\n",
        "load_in_4bit = True\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "# LoRA Adapters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        ")\n",
        "\n",
        "print(f\"Model {model_name} loaded successfully with LoRA.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Bq6O41sdEa6D",
      "metadata": {
        "id": "Bq6O41sdEa6D"
      },
      "outputs": [],
      "source": [
        "BASE_CONFIG = {\n",
        "    'per_device_train_batch_size': 2,\n",
        "    'gradient_accumulation_steps': 4,\n",
        "    'warmup_ratio': 0.1,\n",
        "    'num_train_epochs': 3,\n",
        "    'learning_rate': 5e-5,\n",
        "    'logging_steps': 1,\n",
        "    'optim': \"paged_adamw_32bit\",\n",
        "    'weight_decay': 0.01,\n",
        "    'lr_scheduler_type': \"linear\",\n",
        "    'beta': 0.2,\n",
        "    'max_prompt_length': 512,\n",
        "    'max_length': 1024,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BZDlbytpR16w",
      "metadata": {
        "id": "BZDlbytpR16w"
      },
      "outputs": [],
      "source": [
        "PatchDPOTrainer()\n",
        "\n",
        "training_args = DPOConfig(\n",
        "    per_device_train_batch_size = BASE_CONFIG['per_device_train_batch_size'],\n",
        "    gradient_accumulation_steps = BASE_CONFIG['gradient_accumulation_steps'],\n",
        "    warmup_ratio                = BASE_CONFIG['warmup_ratio'],\n",
        "    num_train_epochs            = BASE_CONFIG['num_train_epochs'],\n",
        "    learning_rate               = BASE_CONFIG['learning_rate'],\n",
        "    fp16                        = not torch.cuda.is_bf16_supported(),\n",
        "    bf16                        = torch.cuda.is_bf16_supported(),\n",
        "    logging_steps               = 1,\n",
        "    optim                       = BASE_CONFIG['optim'],\n",
        "    weight_decay                = BASE_CONFIG['weight_decay'],\n",
        "    lr_scheduler_type           = BASE_CONFIG['lr_scheduler_type'],\n",
        "    seed                        = SEED,\n",
        "    output_dir                  = \"outputs\",\n",
        "    eval_strategy               = \"steps\",\n",
        "    eval_steps                  = 10,\n",
        "    report_to                   = \"none\",\n",
        "\n",
        "    # DPO specific\n",
        "    beta                        = BASE_CONFIG['beta'],\n",
        "    max_prompt_length           = BASE_CONFIG['max_prompt_length'],\n",
        "    max_length                  = BASE_CONFIG['max_length'],\n",
        ")\n",
        "\n",
        "dpo_trainer = DPOTrainer(\n",
        "    model = model,\n",
        "    ref_model = None,\n",
        "    args = training_args,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = eval_dataset,\n",
        "    tokenizer = tokenizer,\n",
        "    beta = 0.1,\n",
        "    max_prompt_length = 512,\n",
        "    max_length = 1024,\n",
        ")\n",
        "\n",
        "print(\"--- Training Started ---\")\n",
        "dpo_trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "te9liR8uFdJ1",
      "metadata": {
        "id": "te9liR8uFdJ1"
      },
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "sim_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "questions = eval_df_raw[\"Question\"].tolist()\n",
        "prompts = [f\"Question: {q}\\nResponse:\" for q in questions]\n",
        "\n",
        "print(f\"Generating responses for {len(questions)} questions...\")\n",
        "inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
        "decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "# Clean up responses\n",
        "model_results = [text.split(\"Response:\")[-1].strip() for text in decoded_outputs]\n",
        "eval_df_raw[\"Model_Result\"] = model_results\n",
        "\n",
        "print(\"Start Cosine Similarity Calculation\")\n",
        "model_embeddings = sim_model.encode(eval_df_raw[\"Model_Result\"].tolist(), convert_to_tensor=True)\n",
        "polite_embeddings = sim_model.encode(eval_df_raw[\"Polite\"].tolist(), convert_to_tensor=True)\n",
        "\n",
        "# how close the model got to the \"Polite\" target\n",
        "cosine_scores = util.cos_sim(model_embeddings, polite_embeddings)\n",
        "eval_df_raw[\"Similarity_Score\"] = torch.diag(cosine_scores).cpu().tolist()\n",
        "\n",
        "eval_df_raw.to_csv(\"mscaidl-0077_task_5_results.csv\", index=False)\n",
        "\n",
        "avg_sim = eval_df_raw['Similarity_Score'].mean()\n",
        "print(f\"Process Complete.\")\n",
        "print(f\"Average Similarity Score: {avg_sim:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FG-3AnAKYtOl",
      "metadata": {
        "id": "FG-3AnAKYtOl"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained_merged(\"dpo_ramsay_model\", tokenizer, save_method = \"merged_16bit\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uCAEKf-QaTag",
      "metadata": {
        "id": "uCAEKf-QaTag"
      },
      "outputs": [],
      "source": [
        "# this step takes ~15 mins because it saves the full model ~5GB\n",
        "# we could save only the changes we made at the DPO step\n",
        "\n",
        "# shutil.make_archive(\"dpo_ramsay_model\", 'zip', \"dpo_ramsay_model\")\n",
        "\n",
        "# files.download(\"dpo_ramsay_model.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TjM6bBDndo9X",
      "metadata": {
        "id": "TjM6bBDndo9X"
      },
      "outputs": [],
      "source": [
        "# LoRA save only\n",
        "\n",
        "model.save_pretrained(\"dpo_ramsay_lora_only\")\n",
        "tokenizer.save_pretrained(\"dpo_ramsay_lora_only\")\n",
        "\n",
        "shutil.make_archive(\"dpo_ramsay_lora_only\", 'zip', \"dpo_ramsay_lora_only\")\n",
        "\n",
        "files.download(\"dpo_ramsay_lora_only.zip\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv_msc",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}