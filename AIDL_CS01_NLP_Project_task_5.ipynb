{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "225227e1",
      "metadata": {
        "id": "225227e1"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tcharos/NLP-Toxicity-Detection/blob/main/AIDL_CS01_NLP_Project_task_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rekBaK9gR16r",
      "metadata": {
        "id": "rekBaK9gR16r"
      },
      "source": [
        "# AIDL_B_CS01: Advanced NLP Project\n",
        "\n",
        "## LLM Tuning with DPO (Gordon Ramsay Alignment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "dc91b44f",
      "metadata": {
        "id": "dc91b44f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running locally. Checking Mac-specific requirements...\n",
            "Requirement already satisfied: unsloth-mlx in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (0.3.2)\n",
            "Requirement already satisfied: mlx>=0.20.0 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from unsloth-mlx) (0.29.3)\n",
            "Requirement already satisfied: mlx-lm>=0.18.0 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from unsloth-mlx) (0.29.1)\n",
            "Requirement already satisfied: transformers>=4.36.0 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from unsloth-mlx) (4.57.3)\n",
            "Requirement already satisfied: tokenizers>=0.15.0 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from unsloth-mlx) (0.22.1)\n",
            "Requirement already satisfied: datasets>=2.14.0 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from unsloth-mlx) (4.4.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from unsloth-mlx) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from unsloth-mlx) (1.26.4)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from unsloth-mlx) (4.66.4)\n",
            "Requirement already satisfied: filelock in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from datasets>=2.14.0->unsloth-mlx) (3.19.1)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from datasets>=2.14.0->unsloth-mlx) (21.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from datasets>=2.14.0->unsloth-mlx) (0.4.0)\n",
            "Requirement already satisfied: pandas in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from datasets>=2.14.0->unsloth-mlx) (2.2.1)\n",
            "Requirement already satisfied: requests>=2.32.2 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from datasets>=2.14.0->unsloth-mlx) (2.32.3)\n",
            "Requirement already satisfied: httpx<1.0.0 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from datasets>=2.14.0->unsloth-mlx) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from datasets>=2.14.0->unsloth-mlx) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.19 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from datasets>=2.14.0->unsloth-mlx) (0.70.18)\n",
            "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.0->unsloth-mlx) (2025.10.0)\n",
            "Requirement already satisfied: packaging in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from datasets>=2.14.0->unsloth-mlx) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from datasets>=2.14.0->unsloth-mlx) (6.0.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.0->unsloth-mlx) (3.13.2)\n",
            "Requirement already satisfied: anyio in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from httpx<1.0.0->datasets>=2.14.0->unsloth-mlx) (4.12.0)\n",
            "Requirement already satisfied: certifi in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from httpx<1.0.0->datasets>=2.14.0->unsloth-mlx) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from httpx<1.0.0->datasets>=2.14.0->unsloth-mlx) (1.0.9)\n",
            "Requirement already satisfied: idna in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from httpx<1.0.0->datasets>=2.14.0->unsloth-mlx) (3.7)\n",
            "Requirement already satisfied: h11>=0.16 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.14.0->unsloth-mlx) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->unsloth-mlx) (4.11.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->unsloth-mlx) (1.2.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.0->unsloth-mlx) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.0->unsloth-mlx) (1.4.0)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.0->unsloth-mlx) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.0->unsloth-mlx) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.0->unsloth-mlx) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.0->unsloth-mlx) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.0->unsloth-mlx) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.0->unsloth-mlx) (1.22.0)\n",
            "Requirement already satisfied: mlx-metal==0.29.3 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from mlx>=0.20.0->unsloth-mlx) (0.29.3)\n",
            "Requirement already satisfied: sentencepiece in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from mlx-lm>=0.18.0->unsloth-mlx) (0.2.1)\n",
            "Requirement already satisfied: protobuf in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from mlx-lm>=0.18.0->unsloth-mlx) (4.25.8)\n",
            "Requirement already satisfied: jinja2 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from mlx-lm>=0.18.0->unsloth-mlx) (3.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from requests>=2.32.2->datasets>=2.14.0->unsloth-mlx) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from requests>=2.32.2->datasets>=2.14.0->unsloth-mlx) (2.2.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from transformers>=4.36.0->unsloth-mlx) (2025.11.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from transformers>=4.36.0->unsloth-mlx) (0.7.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from anyio->httpx<1.0.0->datasets>=2.14.0->unsloth-mlx) (1.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from jinja2->mlx-lm>=0.18.0->unsloth-mlx) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from pandas->datasets>=2.14.0->unsloth-mlx) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from pandas->datasets>=2.14.0->unsloth-mlx) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from pandas->datasets>=2.14.0->unsloth-mlx) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /Users/thodorischaros/Documents/MSc/venv_msc/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.14.0->unsloth-mlx) (1.16.0)\n",
            "\n",
            "TensorFlow Version: 2.16.2\n",
            "Num GPUs Available (TF):  0\n",
            "PyTorch Device: Mac GPU (Metal)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "BASE_DIR = \"/content\" if IN_COLAB else \".\"\n",
        "TOXICITY_PATH = os.path.join(BASE_DIR, \"data_sets/toxicity\")\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"Running in Google Colab. Installing NLP stack...\")\n",
        "    !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "    !pip install -q -U transformers datasets accelerate peft trl sentence-transformers\n",
        "else:\n",
        "    print(\"Running locally. Checking Mac-specific requirements...\")\n",
        "    !{sys.executable} -m pip install -q \"tensorflow==2.16.2\" \"tensorflow-macos==2.16.2\" \"tf-keras~=2.16\"\n",
        "    !{sys.executable} -m pip install unsloth-mlx\n",
        "    !{sys.executable} -m pip install -q -U transformers datasets accelerate peft trl sentence-transformers\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "from datasets import Dataset\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import DPOTrainer\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "if IN_COLAB:\n",
        "    from unsloth import FastLanguageModel\n",
        "else:\n",
        "    from unsloth_mlx import FastLanguageModel\n",
        "\n",
        "print(f\"\\nTensorFlow Version: {tf.__version__}\")\n",
        "print(\"Num GPUs Available (TF): \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "HAS_MPS = torch.backends.mps.is_available()\n",
        "HAS_CUDA = torch.cuda.is_available()\n",
        "\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print(\"PyTorch Device: Mac GPU (Metal)\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"PyTorch Device: Colab GPU (CUDA)\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"PyTorch Device: CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e548f0da",
      "metadata": {},
      "source": [
        "### Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0fc76ac2",
      "metadata": {
        "id": "0fc76ac2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Could not load ./data_sets/Ramsay/test/ramsay_mscaidl_0115.csv due to error: Error tokenizing data. C error: Expected 3 fields in line 4, saw 4\n",
            "\n",
            "Could not load ./data_sets/Ramsay/test/CS02_Task4_msaidl_0089.csv due to error: Error tokenizing data. C error: Expected 4 fields in line 72, saw 5\n",
            "\n",
            "Skipping ./data_sets/Ramsay/test/RamsayQ_A.csv: Missing required columns. Found: ['AIDL_ID', 'student_question', 'polite_answer', 'ramsay_answer']\n",
            "Could not load ./data_sets/Ramsay/test/0067_Preference Alignment Dataset.csv due to error: Error tokenizing data. C error: Expected 2 fields in line 3, saw 3\n",
            "\n",
            "Skipping ./data_sets/Ramsay/test/Preference Alignment Dataset_mscaidl-0122.csv: Missing required columns. Found: ['ID', 'Question', 'Polite Answer', 'Gordon Ramsay Style (Toxic)']\n",
            "Could not load ./data_sets/Ramsay/test/Toxic Dataset.csv due to error: Error tokenizing data. C error: Expected 1 fields in line 22, saw 2\n",
            "\n",
            "Skipping ./data_sets/Ramsay/test/gordon_ramsay_prompt_engineering.csv: Missing required columns. Found: ['question', 'style', 'answer']\n",
            "Skipping ./data_sets/Ramsay/test/ramsay_100_examples_0100.csv: Missing required columns. Found: ['\"AIDL_ID, Question ,Polite ,Ramsay\"']\n",
            "Could not load ./data_sets/Ramsay/test/CS01-MSCAIDL0088_QUST-polite-ramsey_UTF-8.csv due to error: Error tokenizing data. C error: Expected 3 fields in line 51, saw 5\n",
            "\n",
            "Skipping ./data_sets/Ramsay/test/database NLP.csv: Missing required columns. Found: ['Questions', 'Polite Answer', 'Ramsay-style answer']\n",
            "Could not load ./data_sets/Ramsay/test/Bertsia_Foivi_mscaidl-0110_ramsay_dataset.csv due to error: 'charmap' codec can't decode byte 0x9d in position 16: character maps to <undefined>\n",
            "Skipping ./data_sets/Ramsay/test/Gordon Ramsay csv AIDL_0066.csv: Missing required columns. Found: ['0066', 'Why do we use ReLU?', 'ReLU prevents vanishing gradients.', 'ReLU keeps gradients alive unlike you killing every brain cell in a 5 mile radius you donkey!']\n",
            "Skipping ./data_sets/Ramsay/test/nlp.csv: Missing required columns. Found: ['aidl_id;student_question;polite_answer;ramsay_answer']\n",
            "Successfully loaded 19 files.\n",
            "Total training rows available: 1900\n"
          ]
        }
      ],
      "source": [
        "# Path to the folder containing colleagues' CSVs\n",
        "test_folder_path = './data_sets/Ramsay/test'\n",
        "all_csv_files = glob.glob(os.path.join(test_folder_path, \"*.csv\"))\n",
        "\n",
        "valid_dfs = []\n",
        "required_cols = [\"Question\", \"Polite\", \"Ramsay\"]\n",
        "\n",
        "for f in all_csv_files:\n",
        "    try:\n",
        "        # Try UTF-8 first, fallback to cp1252 if it fails\n",
        "        try:\n",
        "            temp_df = pd.read_csv(f, encoding='utf-8')\n",
        "        except UnicodeDecodeError:\n",
        "            temp_df = pd.read_csv(f, encoding='cp1252')\n",
        "        \n",
        "        # Check if the required columns exist\n",
        "        if all(col in temp_df.columns for col in required_cols):\n",
        "            valid_dfs.append(temp_df[required_cols])\n",
        "        else:\n",
        "            print(f\"Skipping {f}: Missing required columns. Found: {temp_df.columns.tolist()}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Could not load {f} due to error: {e}\")\n",
        "\n",
        "# Combine only the valid ones\n",
        "if valid_dfs:\n",
        "    all_colleagues_data = pd.concat(valid_dfs, ignore_index=True)\n",
        "    # Requirement 4: Save to test.csv\n",
        "    all_colleagues_data.to_csv(\"test.csv\", index=False)\n",
        "    \n",
        "    # Take 500 for training\n",
        "    train_df = all_colleagues_data.sample(n=min(500, len(all_colleagues_data)), random_state=42)\n",
        "    print(f\"Successfully loaded {len(valid_dfs)} files.\")\n",
        "    print(f\"Total training rows available: {len(all_colleagues_data)}\")\n",
        "else:\n",
        "    print(\"No valid CSV files were loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "00b29b44",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                      AIDL_ID;Question;Polite;Ramsay\n",
            "0  0077;\"What is the purpose of a Padding token?\"...\n",
            "1  0077;\"What is the purpose of Dropout?\";\"Dropou...\n",
            "2  0077;\"Why normalize input data?\";\"Normalizatio...\n",
            "3  0077;\"What is a Learning Rate?\";\"It is a hyper...\n",
            "4  0077;\"Why use Batch Normalization?\";\"It stabil...\n"
          ]
        }
      ],
      "source": [
        "val_folder_path = './data_sets/Ramsay/val'\n",
        "your_csv_file = glob.glob(os.path.join(val_folder_path, \"mscaidl-0077_ramsay_dataset.csv\"))[0]\n",
        "\n",
        "eval_df = pd.read_csv(your_csv_file)\n",
        "print(eval_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "814025e8",
      "metadata": {
        "id": "814025e8"
      },
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c071abdb",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "NIKkbtJmR16w",
      "metadata": {
        "id": "NIKkbtJmR16w"
      },
      "source": [
        "### SLM from usloath (not Zephyr)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g7Vu9BRfR16w",
      "metadata": {
        "id": "g7Vu9BRfR16w"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching 9 files:  33%|███▎      | 3/9 [01:13<02:26, 24.37s/it]\n",
            "Cancellation requested; stopping current tasks.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m load_in_4bit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;66;03m# Essential for DPO memory efficiency\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 2. Load the model and tokenizer\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mFastLanguageModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# 3. Add LoRA Adapters\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# This allows us to train the model efficiently by only updating a small percentage of weights\u001b[39;00m\n\u001b[1;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m FastLanguageModel\u001b[38;5;241m.\u001b[39mget_peft_model(\n\u001b[1;32m     17\u001b[0m     model,\n\u001b[1;32m     18\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m, \u001b[38;5;66;03m# Rank\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3407\u001b[39m,\n\u001b[1;32m     26\u001b[0m )\n",
            "File \u001b[0;32m~/Documents/MSc/venv_msc/lib/python3.9/site-packages/unsloth_mlx/model.py:131\u001b[0m, in \u001b[0;36mFastLanguageModel.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m mlx_kwargs\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# Load model using MLX (with config for saving later)\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m     model, tokenizer, config \u001b[38;5;241m=\u001b[39m \u001b[43mmlx_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmlx_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# Wrap model with our compatibility layer\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     wrapped_model \u001b[38;5;241m=\u001b[39m MLXModelWrapper(\n\u001b[1;32m    135\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    136\u001b[0m         tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m    140\u001b[0m     )\n",
            "File \u001b[0;32m~/Documents/MSc/venv_msc/lib/python3.9/site-packages/mlx_lm/utils.py:320\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path_or_hf_repo, tokenizer_config, model_config, adapter_path, lazy, return_config, revision)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m    285\u001b[0m     path_or_hf_repo: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    286\u001b[0m     tokenizer_config: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    294\u001b[0m     Tuple[nn\u001b[38;5;241m.\u001b[39mModule, TokenizerWrapper, Dict[\u001b[38;5;28mstr\u001b[39m, Any]],\n\u001b[1;32m    295\u001b[0m ]:\n\u001b[1;32m    296\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;124;03m    Load the model and tokenizer from a given path or a huggingface repository.\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;124;03m        ValueError: If model class or args class are not found.\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m \u001b[43m_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_hf_repo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m     model, config \u001b[38;5;241m=\u001b[39m load_model(model_path, lazy, model_config\u001b[38;5;241m=\u001b[39mmodel_config)\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m adapter_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/Documents/MSc/venv_msc/lib/python3.9/site-packages/mlx_lm/utils.py:133\u001b[0m, in \u001b[0;36m_download\u001b[0;34m(path_or_hf_repo, revision, allow_patterns)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m    121\u001b[0m     allow_patterns \u001b[38;5;241m=\u001b[39m allow_patterns \u001b[38;5;129;01mor\u001b[39;00m [\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel*.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.jinja\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    131\u001b[0m     ]\n\u001b[1;32m    132\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m Path(\n\u001b[0;32m--> 133\u001b[0m         \u001b[43msnapshot_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath_or_hf_repo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_patterns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_patterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m     )\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_path\n",
            "File \u001b[0;32m~/Documents/MSc/venv_msc/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/MSc/venv_msc/lib/python3.9/site-packages/huggingface_hub/_snapshot_download.py:332\u001b[0m, in \u001b[0;36msnapshot_download\u001b[0;34m(repo_id, repo_type, revision, cache_dir, local_dir, library_name, library_version, user_agent, proxies, etag_timeout, force_download, token, local_files_only, allow_patterns, ignore_patterns, max_workers, tqdm_class, headers, endpoint, local_dir_use_symlinks, resume_download)\u001b[0m\n\u001b[1;32m    330\u001b[0m         _inner_hf_hub_download(file)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 332\u001b[0m     \u001b[43mthread_map\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_inner_hf_hub_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiltered_repo_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtqdm_desc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# User can use its own tqdm class or the default one from `huggingface_hub.utils`\u001b[39;49;00m\n\u001b[1;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtqdm_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhf_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(local_dir))\n",
            "File \u001b[0;32m~/Documents/MSc/venv_msc/lib/python3.9/site-packages/tqdm/contrib/concurrent.py:69\u001b[0m, in \u001b[0;36mthread_map\u001b[0;34m(fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03mEquivalent of `list(map(fn, *iterables))`\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03mdriven by `concurrent.futures.ThreadPoolExecutor`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    [default: max(32, cpu_count() + 4)].\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconcurrent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfutures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_executor_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtqdm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/MSc/venv_msc/lib/python3.9/site-packages/tqdm/contrib/concurrent.py:51\u001b[0m, in \u001b[0;36m_executor_map\u001b[0;34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ensure_lock(tqdm_class, lock_name\u001b[38;5;241m=\u001b[39mlock_name) \u001b[38;5;28;01mas\u001b[39;00m lk:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# share lock in case workers are already using `tqdm`\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m PoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mmax_workers, initializer\u001b[38;5;241m=\u001b[39mtqdm_class\u001b[38;5;241m.\u001b[39mset_lock,\n\u001b[1;32m     50\u001b[0m                       initargs\u001b[38;5;241m=\u001b[39m(lk,)) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m---> 51\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/MSc/venv_msc/lib/python3.9/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
            "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:608\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    606\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 608\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    610\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mresult(end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
            "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:440\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 440\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
            "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# model_name = \"unsloth/Llama-3.2-3B-Instruct\"\n",
        "model_name = \"unsloth/Llama-3.2-1B-Instruct\"\n",
        "\n",
        "max_seq_length = 2048 # Can handle longer contexts if needed\n",
        "dtype = None # Auto-detect (Float16 or Bfloat16)\n",
        "load_in_4bit = True # Essential for DPO memory efficiency\n",
        "\n",
        "# 2. Load the model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "# 3. Add LoRA Adapters\n",
        "# This allows us to train the model efficiently by only updating a small percentage of weights\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Rank\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Optimized for Unsloth\n",
        "    bias = \"none\",    # Optimized for Unsloth\n",
        "    use_gradient_checkpointing = \"unsloth\", # Reduces VRAM usage\n",
        "    random_state = 3407,\n",
        ")\n",
        "\n",
        "print(f\"Model {model_name} loaded successfully with LoRA adapters.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BZDlbytpR16w",
      "metadata": {
        "id": "BZDlbytpR16w"
      },
      "outputs": [],
      "source": [
        "### 5.2. Model and DPO Trainer Setup\n",
        "\n",
        "# **ACTION REQUIRED: Choose your base model**\n",
        "BASE_MODEL = \"facebook/opt-125m\" # Use a small model for development, or a larger one if resources allow\n",
        "\n",
        "# 1. Load the base model and tokenizer\n",
        "# model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, torch_dtype=torch.bfloat16) # Use a suitable dtype\n",
        "# tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
        "# tokenizer.pad_token = tokenizer.eos_token # Ensure pad token is set\n",
        "\n",
        "# 2. Setup PEFT/LoRA configuration\n",
        "# peft_config = LoraConfig(\n",
        "#     r=16,\n",
        "#     lora_alpha=16,\n",
        "#     target_modules=[\"q_proj\", \"v_proj\"], # Check model documentation for correct layers\n",
        "#     lora_dropout=0.05,\n",
        "#     bias=\"none\",\n",
        "#     task_type=\"CAUSAL_LM\",\n",
        "# )\n",
        "\n",
        "# 3. DPO Training Arguments\n",
        "# training_args_dpo = TrainingArguments(\n",
        "#     output_dir=\"./dpo_results_ramsay\",\n",
        "#     num_train_epochs=1,\n",
        "#     per_device_train_batch_size=4,\n",
        "#     gradient_accumulation_steps=4,\n",
        "#     logging_steps=10,\n",
        "#     learning_rate=5e-5,\n",
        "#     remove_unused_columns=False,\n",
        "#     save_strategy=\"epoch\",\n",
        "#     fp16=True, # Use fp16/bf16 if supported\n",
        "#     report_to=\"none\"\n",
        "# )\n",
        "\n",
        "# 4. Initialize DPOTrainer\n",
        "# dpo_trainer = DPOTrainer(\n",
        "#     model=model,\n",
        "#     ref_model=None, # Set to None for implicit reference model loading\n",
        "#     args=training_args_dpo,\n",
        "#     beta=0.1,\n",
        "#     train_dataset=dpo_hf_dataset['train'],\n",
        "#     eval_dataset=dpo_hf_dataset['test'],\n",
        "#     tokenizer=tokenizer,\n",
        "#     peft_config=peft_config,\n",
        "# )\n",
        "\n",
        "# 5. Train Placeholder\n",
        "# print(\"Starting DPO Training...\")\n",
        "# dpo_trainer.train()\n",
        "\n",
        "# 6. Save the final model (LoRA weights)\n",
        "# dpo_trainer.save_model(\"ramsay_dpo_adapter\")\n",
        "\n",
        "# **ACTION REQUIRED: Inference Test**\n",
        "# Test the fine-tuned model with a new question to verify the Ramsay persona.\n",
        "# from peft import PeftModel\n",
        "# ft_model = PeftModel.from_pretrained(model, \"ramsay_dpo_adapter\")\n",
        "# ft_model.eval()\n",
        "# print(\"\\n--- DPO Fine-Tuned Model Test ---\")\n",
        "# test_prompt = \"Why is batch normalization useful?\"\n",
        "# ... generate response ...\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv_msc",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
