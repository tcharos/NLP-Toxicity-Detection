{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tcharos/NLP-Toxicity-Detection/blob/main/AIDL_CS01_NLP_Project_task_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMDT4GCHCpMd"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install langchain-community langchain-text-splitters pypdf sentence-transformers numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVGq3zaTo_XY"
      },
      "outputs": [],
      "source": [
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oBBU4LkoGmB"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nx5KfV0S7Nt2"
      },
      "source": [
        "### 1. Load and segment a PDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmcmf_3s7FKy"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_Zy8pNDVAj_"
      },
      "outputs": [],
      "source": [
        "online_document = \"https://academicweb.nd.edu/~lemmon/courses/deep-learning/lecture-book/deep-learning-book-2025.pdf\"\n",
        "\n",
        "loader = PyPDFLoader(online_document) # define loader\n",
        "pages = loader.load() # load\n",
        "\n",
        "print(f\"Loaded {len(pages)} pages\")\n",
        "print(f\"First page preview:\\n{pages[0].page_content[:500]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kj5WRbQcD0rE"
      },
      "outputs": [],
      "source": [
        "# Split into chunks\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=20,\n",
        "    length_function=len\n",
        ") # define splitter with chunk size 1000 and 20% overlap\n",
        "\n",
        "chunks = splitter.split_documents(pages)\n",
        "\n",
        "print(f\"\\nCreated {len(chunks)} chunks\")\n",
        "print(f\"First chunk:\\n{chunks[0].page_content}\")\n",
        "print(f\"Metadata: {chunks[0].metadata}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWF8OFkZ8F1h"
      },
      "source": [
        "### 2. Embed and store in numpy array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqTh0DJoD80f"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "model_emb = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Extract text and generate embeddings\n",
        "texts = [doc.page_content for doc in chunks]\n",
        "embeddings_array = model_emb.encode(texts, convert_to_numpy=True)\n",
        "\n",
        "# Save and reload example\n",
        "np.save('embeddings.npy', embeddings_array)\n",
        "embeddings_array = np.load('embeddings.npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yb9fXTUS8tjn"
      },
      "source": [
        "### 3. Use an S/LLM from Hugginface for paraphrase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7kZRU6-871s"
      },
      "outputs": [],
      "source": [
        "# use a model, check how to do that in HF, for example check this model card https://huggingface.co/google/gemma-2b\n",
        "#  or select on from unsloath\n",
        "# paraphrase 2 times (create the necessary prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXqr7VGvCl9t",
        "outputId": "6170d0ff-95f8-44e3-8885-fb8cda146411"
      },
      "outputs": [],
      "source": [
        "# Load model and tokenizer\n",
        "model_llm, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "FastLanguageModel.for_inference(model_llm)\n",
        "\n",
        "def get_paraphrases(question, n=2):\n",
        "    prompt = f\"Paraphrase the following question in {n} different ways. Return only the paraphrased questions, one per line:\\n\\nQuestion: {question}\"\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model_llm.generate(**inputs, max_new_tokens=100)\n",
        "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "    # split by newline\n",
        "    return [question] + decoded.split('\\n')[-n:]\n",
        "\n",
        "question = \"what is an RNN?\"\n",
        "paraphrased_queries = get_paraphrases(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1h7MBjRRnMMo",
        "outputId": "4a330615-adcc-42ea-cfd9-53b82742bcb1"
      },
      "outputs": [],
      "source": [
        "print(\"--- Paraphrased Qustion ---\")\n",
        "for i, query in enumerate(paraphrased_queries, 1):\n",
        "    print(f\"Paraphrase {i}: {query}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L00SX0vG-f1m"
      },
      "source": [
        "### 4. Retrieve 5 most semantically close chunks (cosine similarity) for every paraphrase*, then add threshold 0.3 and select top 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MI38y5WpEqfp"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "all_indices = []\n",
        "threshold = 0.3\n",
        "\n",
        "for q in paraphrased_queries:\n",
        "    q_emb = model_emb.encode([q])\n",
        "    sims = cosine_similarity(q_emb, embeddings_array)[0]\n",
        "\n",
        "    top_5_idx = sims.argsort()[-5:][::-1]\n",
        "    filtered_idx = [i for i in top_5_idx if sims[i] >= threshold]\n",
        "    all_indices.extend(filtered_idx)\n",
        "\n",
        "unique_top_indices = list(dict.fromkeys(all_indices))[:3]\n",
        "retrieved_context = \"\\n\".join([chunks[i].page_content for i in unique_top_indices])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyLRUrfNnjhf",
        "outputId": "9f98c09a-069f-4198-9a62-cfaa705b1990"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "all_queries = [question] + paraphrased_queries\n",
        "candidate_pool = []\n",
        "threshold = 0.3\n",
        "\n",
        "print(f\"--- Verification: Searching for {len(all_queries)} query variations ---\")\n",
        "\n",
        "for q in all_queries:\n",
        "    q_emb = model_emb.encode([q])\n",
        "\n",
        "    sims = cosine_similarity(q_emb, embeddings_array)[0]\n",
        "\n",
        "    top_5_idx = sims.argsort()[-5:][::-1]\n",
        "\n",
        "    for idx in top_5_idx:\n",
        "        score = sims[idx]\n",
        "        if score >= threshold:\n",
        "            candidate_pool.append((idx, score))\n",
        "            print(f\"Match Found! Query: '{q[:30]}...' | Score: {score:.4f} | Index: {idx}\")\n",
        "\n",
        "candidate_pool.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "final_indices = []\n",
        "seen_indices = set()\n",
        "\n",
        "for idx, score in candidate_pool:\n",
        "    if idx not in seen_indices:\n",
        "        final_indices.append(idx)\n",
        "        seen_indices.add(idx)\n",
        "    if len(final_indices) == 3: # get top 3\n",
        "        break\n",
        "\n",
        "print(f\"\\n--- Final Selection (Top {len(final_indices)} Unique Chunks) ---\")\n",
        "for i, idx in enumerate(final_indices):\n",
        "    content_preview = chunks[idx].page_content[:150].replace('\\n', ' ')\n",
        "    print(f\"{i+1}. [Index {idx}] Content: {content_preview}...\")\n",
        "\n",
        "# Store for 5\n",
        "retrieved_context = \"\\n\\n\".join([chunks[i].page_content for i in final_indices])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuPR9KABxyRe"
      },
      "outputs": [],
      "source": [
        "# Re-ranking (not mandatory)\n",
        "\n",
        "# ----------- Disclaimer - Not mine code (AI generated) but I wanted to try it out -----------\n",
        "\n",
        "# from sentence_transformers import CrossEncoder\n",
        "\n",
        "# rerank_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "# original_query = all_queries[0]\n",
        "# pairs = [[original_query, chunk] for chunk in retrieved_context]\n",
        "\n",
        "# rerank_scores = rerank_model.predict(pairs)\n",
        "\n",
        "# reranked_results = sorted(\n",
        "#     zip(retrieved_context, rerank_scores),\n",
        "#     key=lambda x: x[1],\n",
        "#     reverse=True\n",
        "# )\n",
        "\n",
        "# final_retrieved_context = [chunk for chunk, score in reranked_results[:3]]\n",
        "\n",
        "# print(\"Re-ranking complete. Chunks are now ordered by relevance to the original question.\")\n",
        "\n",
        "# ----------- Disclaimer - Not mine code (AI generated) -----------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDQpyIPO-vCV"
      },
      "source": [
        "### 5. Apply the augmentation phase using an SLM (could be the one you have used in step 2 or the only aligned with DPO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSrJkh_gFJgy",
        "outputId": "cd5fc168-6b9e-4c5c-db43-dbae2f7ac86d"
      },
      "outputs": [],
      "source": [
        "final_prompt = f\"\"\"Use the following context to answer the question.\n",
        "Context: {retrieved_context}\n",
        "Question: {paraphrased_queries[0]}\n",
        "Answer:\"\"\"\n",
        "\n",
        "inputs = tokenizer([final_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model_llm.generate(**inputs, max_new_tokens=250)\n",
        "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
