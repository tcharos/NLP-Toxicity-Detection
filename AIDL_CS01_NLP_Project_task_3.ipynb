{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tcharos/NLP-Toxicity-Detection/blob/main/AIDL_CS01_NLP_Project_task_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMDT4GCHCpMd"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install langchain-community langchain-text-splitters pypdf sentence-transformers numpy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nx5KfV0S7Nt2"
      },
      "source": [
        "### 1. Load and segment a PDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmcmf_3s7FKy"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_Zy8pNDVAj_"
      },
      "outputs": [],
      "source": [
        "online_document = \"https://academicweb.nd.edu/~lemmon/courses/deep-learning/lecture-book/deep-learning-book-2025.pdf\"\n",
        "\n",
        "loader = ... # define loader\n",
        "pages = ... # load\n",
        "\n",
        "print(f\"Loaded {len(pages)} pages\")\n",
        "print(f\"First page preview:\\n{pages[0].page_content[:500]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kj5WRbQcD0rE"
      },
      "outputs": [],
      "source": [
        "# Split into chunks\n",
        "splitter = ... # define splitter with chunk size 1000 and 20% overlap\n",
        "\n",
        "print(f\"\\nCreated {len(chunks)} chunks\")\n",
        "print(f\"First chunk:\\n{chunks[0].page_content}\")\n",
        "print(f\"Metadata: {chunks[0].metadata}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWF8OFkZ8F1h"
      },
      "source": [
        "### 2. Embed and store in numpy array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqTh0DJoD80f"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "# Load an open embedding model (runs locally, no API key needed)\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Extract text from chunks\n",
        "texts = ...\n",
        "\n",
        "# Generate embeddings\n",
        "embeddings = ...\n",
        "\n",
        "# Convert to numpy array (it's probably already a numpy array, but being explicit)\n",
        "embeddings_array = ....\n",
        "\n",
        "# Save to disk\n",
        "# saving npy code\n",
        "\n",
        "# Load later with:\n",
        "embeddings_array = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yb9fXTUS8tjn"
      },
      "source": [
        "### 3. use an S/LLM from Hugginface for paraphrase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7kZRU6-871s"
      },
      "outputs": [],
      "source": [
        "# use a model, check how to do that in HF, for example check this model card https://huggingface.co/google/gemma-2b\n",
        "#  or select on from unsloath\n",
        "# paraphrase 2 times (create the necessary prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1STTDixp_DK5"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L00SX0vG-f1m"
      },
      "source": [
        "### 4. Retrieve 5 most semantically close chunks (cosine similarity) for every paraphrase*, then add threshold 0.3 and select top 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MI38y5WpEqfp"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Your question\n",
        "question = \"what is an RNN?\" #feel free to play around\n",
        "\n",
        "# Embed the question\n",
        "question_embedding = ...\n",
        "\n",
        "# Calculate cosine similarity between 3 question and all chunks\n",
        "similarities = ...\n",
        "\n",
        "# Get top 5 indices sorted by similarity for each 3 questions (1 + 2 x paraphrased)\n",
        "\n",
        "\n",
        "# select top\n",
        "\n",
        "# Filter by threshold and retrieve results\n",
        "threshold = ...\n",
        "for ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDQpyIPO-vCV"
      },
      "source": [
        "### 5. Apply the augmentation phase using an SLM (could be the one you have used in step 2 or the only aligned with DPO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSrJkh_gFJgy"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "...."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoIIZOWMd5KV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMiWKtDQi6tEogU95CkQQYW",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
