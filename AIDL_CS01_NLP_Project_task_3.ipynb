{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tcharos/NLP-Toxicity-Detection/blob/main/AIDL_CS01_NLP_Project_task_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMDT4GCHCpMd"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install langchain-community langchain-text-splitters pypdf sentence-transformers numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVGq3zaTo_XY"
      },
      "outputs": [],
      "source": [
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oBBU4LkoGmB"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nx5KfV0S7Nt2"
      },
      "source": [
        "### 1. Load and segment a PDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmcmf_3s7FKy"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_Zy8pNDVAj_"
      },
      "outputs": [],
      "source": [
        "online_document = \"https://academicweb.nd.edu/~lemmon/courses/deep-learning/lecture-book/deep-learning-book-2025.pdf\"\n",
        "\n",
        "loader = PyPDFLoader(online_document) # define loader\n",
        "pages = loader.load() # load\n",
        "\n",
        "print(f\"Loaded {len(pages)} pages\")\n",
        "print(f\"First page preview:\\n{pages[0].page_content[:500]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kj5WRbQcD0rE"
      },
      "outputs": [],
      "source": [
        "# Split into chunks\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=20,\n",
        "    length_function=len\n",
        ") # define splitter with chunk size 1000 and 20% overlap\n",
        "\n",
        "chunks = splitter.split_documents(pages)\n",
        "\n",
        "print(f\"\\nCreated {len(chunks)} chunks\")\n",
        "print(f\"First chunk:\\n{chunks[0].page_content}\")\n",
        "print(f\"Metadata: {chunks[0].metadata}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWF8OFkZ8F1h"
      },
      "source": [
        "### 2. Embed and store in numpy array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqTh0DJoD80f"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "model_emb = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Extract text and generate embeddings\n",
        "texts = [doc.page_content for doc in chunks]\n",
        "embeddings_array = model_emb.encode(texts, convert_to_numpy=True)\n",
        "\n",
        "# Save and reload example\n",
        "np.save('embeddings.npy', embeddings_array)\n",
        "embeddings_array = np.load('embeddings.npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yb9fXTUS8tjn"
      },
      "source": [
        "### 3. Use an S/LLM from Hugginface for paraphrase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7kZRU6-871s"
      },
      "outputs": [],
      "source": [
        "# use a model, check how to do that in HF, for example check this model card https://huggingface.co/google/gemma-2b\n",
        "#  or select on from unsloath\n",
        "# paraphrase 2 times (create the necessary prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXqr7VGvCl9t",
        "outputId": "be1250f5-677b-4b48-c264-0039b1b90728"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2026.1.2: Fast Llama patching. Transformers: 4.57.3.\n",
            "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.161 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "# Load model and tokenizer\n",
        "model_llm, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "FastLanguageModel.for_inference(model_llm)\n",
        "\n",
        "def get_paraphrases(question, n=2):\n",
        "    # Llama 3.2 Chat Template\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": f\"You are a professional assistant. Rewrite the user's question in exactly {n} different, polite, and professional ways. Return ONLY the questions as a numbered list. No notes or extra text.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Question: {question}\"\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = model_llm.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=100,\n",
        "        temperature=0.4,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "    # Split on \"assistant\"\n",
        "    assistant_reply = decoded.split(\"assistant\")[-1].strip()\n",
        "\n",
        "    # Clean the numbering (1., 2.) from each line\n",
        "    cleaned_paraphrases = []\n",
        "    for line in assistant_reply.split('\\n'):\n",
        "        clean = line.lstrip(\"123456789. \").strip()\n",
        "        if clean and len(clean) > 10:\n",
        "            cleaned_paraphrases.append(clean)\n",
        "\n",
        "    return [question] + cleaned_paraphrases[:n]\n",
        "\n",
        "question = \"what is an RNN?\"\n",
        "paraphrased_questions = get_paraphrases(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1h7MBjRRnMMo",
        "outputId": "61d969e2-0145-4019-9154-942a5af520d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Paraphrased Qustion ---\n",
            "Paraphrase 1: what is an RNN?\n",
            "Paraphrase 2: What is the meaning and purpose of a Recurrent Neural Network (RNN)?\n",
            "Paraphrase 3: Can you explain the basic concept and function of a Recurrent Neural Network (RNN)?\n"
          ]
        }
      ],
      "source": [
        "print(\"--- Paraphrased Qustion ---\")\n",
        "for i, query in enumerate(paraphrased_questions, 1):\n",
        "    print(f\"Paraphrase {i}: {query}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L00SX0vG-f1m"
      },
      "source": [
        "### 4. Retrieve 5 most semantically close chunks (cosine similarity) for every paraphrase*, then add threshold 0.3 and select top 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "MI38y5WpEqfp"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "all_indices = []\n",
        "threshold = 0.3\n",
        "\n",
        "for q in paraphrased_questions:\n",
        "    q_emb = model_emb.encode([q])\n",
        "    sims = cosine_similarity(q_emb, embeddings_array)[0]\n",
        "\n",
        "    top_5_idx = sims.argsort()[-5:][::-1]\n",
        "    filtered_idx = [i for i in top_5_idx if sims[i] >= threshold]\n",
        "    all_indices.extend(filtered_idx)\n",
        "\n",
        "unique_top_indices = list(dict.fromkeys(all_indices))[:3]\n",
        "retrieved_context = \"\\n\".join([chunks[i].page_content for i in unique_top_indices])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyLRUrfNnjhf",
        "outputId": "615752f1-e8b1-4364-8df1-c264be1d365d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Verification: Searching for 4 query variations ---\n",
            "Match Found! Query: 'what is an RNN?...' | Score: 0.6279 | Index: 403\n",
            "Match Found! Query: 'what is an RNN?...' | Score: 0.4881 | Index: 407\n",
            "Match Found! Query: 'what is an RNN?...' | Score: 0.4469 | Index: 487\n",
            "Match Found! Query: 'what is an RNN?...' | Score: 0.4154 | Index: 485\n",
            "Match Found! Query: 'what is an RNN?...' | Score: 0.4137 | Index: 416\n",
            "Match Found! Query: 'what is an RNN?...' | Score: 0.6279 | Index: 403\n",
            "Match Found! Query: 'what is an RNN?...' | Score: 0.4881 | Index: 407\n",
            "Match Found! Query: 'what is an RNN?...' | Score: 0.4469 | Index: 487\n",
            "Match Found! Query: 'what is an RNN?...' | Score: 0.4154 | Index: 485\n",
            "Match Found! Query: 'what is an RNN?...' | Score: 0.4137 | Index: 416\n",
            "Match Found! Query: 'What is the meaning and purpos...' | Score: 0.6874 | Index: 403\n",
            "Match Found! Query: 'What is the meaning and purpos...' | Score: 0.5557 | Index: 409\n",
            "Match Found! Query: 'What is the meaning and purpos...' | Score: 0.5414 | Index: 407\n",
            "Match Found! Query: 'What is the meaning and purpos...' | Score: 0.5331 | Index: 416\n",
            "Match Found! Query: 'What is the meaning and purpos...' | Score: 0.4948 | Index: 406\n",
            "Match Found! Query: 'Can you explain the basic conc...' | Score: 0.7205 | Index: 403\n",
            "Match Found! Query: 'Can you explain the basic conc...' | Score: 0.6066 | Index: 409\n",
            "Match Found! Query: 'Can you explain the basic conc...' | Score: 0.5856 | Index: 406\n",
            "Match Found! Query: 'Can you explain the basic conc...' | Score: 0.5478 | Index: 407\n",
            "Match Found! Query: 'Can you explain the basic conc...' | Score: 0.5431 | Index: 416\n",
            "\n",
            "--- Final Selection (Top 3 Unique Chunks) ---\n",
            "1. [Index 403] Content: in more detail. FIGURE 4. Training Curves for model MSE. (left) 1D Con- volutional Model, (middle) Sequential Model, (right) LSTM model) 2. Recurrent ...\n",
            "2. [Index 409] Content: 198 6. DEEP LEARNING FOR NATURAL LANGUAGE PROCESSING Let us consider a simple RNN whose output is computed as h(k) = tanh \u0000 Ux(k) + Wh(k−1) + b \u0001 .(36...\n",
            "3. [Index 406] Content: 2. RECURRENT NEURAL NETWORKS 197 We can also visualize how an RNN computes and trains byunfolding the computation graph on the left of Fig. 5. This un...\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "all_queries = [question] + paraphrased_questions\n",
        "candidate_pool = []\n",
        "threshold = 0.3\n",
        "\n",
        "print(f\"--- Verification: Searching for {len(all_queries)} query variations ---\")\n",
        "\n",
        "for q in all_queries:\n",
        "    q_emb = model_emb.encode([q])\n",
        "\n",
        "    sims = cosine_similarity(q_emb, embeddings_array)[0]\n",
        "\n",
        "    top_5_idx = sims.argsort()[-5:][::-1]\n",
        "\n",
        "    for idx in top_5_idx:\n",
        "        score = sims[idx]\n",
        "        if score >= threshold:\n",
        "            candidate_pool.append((idx, score))\n",
        "            print(f\"Match Found! Query: '{q[:30]}...' | Score: {score:.4f} | Index: {idx}\")\n",
        "\n",
        "candidate_pool.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "final_indices = []\n",
        "seen_indices = set()\n",
        "\n",
        "for idx, score in candidate_pool:\n",
        "    if idx not in seen_indices:\n",
        "        final_indices.append(idx)\n",
        "        seen_indices.add(idx)\n",
        "    if len(final_indices) == 3: # get top 3\n",
        "        break\n",
        "\n",
        "print(f\"\\n--- Final Selection (Top {len(final_indices)} Unique Chunks) ---\")\n",
        "for i, idx in enumerate(final_indices):\n",
        "    content_preview = chunks[idx].page_content[:150].replace('\\n', ' ')\n",
        "    print(f\"{i+1}. [Index {idx}] Content: {content_preview}...\")\n",
        "\n",
        "# Store for 5\n",
        "retrieved_context = \"\\n\\n\".join([chunks[i].page_content for i in final_indices])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuPR9KABxyRe"
      },
      "outputs": [],
      "source": [
        "# Re-ranking (not mandatory)\n",
        "\n",
        "# ----------- Disclaimer - Not mine code (AI generated) but I wanted to try it out -----------\n",
        "\n",
        "# from sentence_transformers import CrossEncoder\n",
        "\n",
        "# rerank_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "# original_query = all_queries[0]\n",
        "# pairs = [[original_query, chunk] for chunk in retrieved_context]\n",
        "\n",
        "# rerank_scores = rerank_model.predict(pairs)\n",
        "\n",
        "# reranked_results = sorted(\n",
        "#     zip(retrieved_context, rerank_scores),\n",
        "#     key=lambda x: x[1],\n",
        "#     reverse=True\n",
        "# )\n",
        "\n",
        "# final_retrieved_context = [chunk for chunk, score in reranked_results[:3]]\n",
        "\n",
        "# print(\"Re-ranking complete. Chunks are now ordered by relevance to the original question.\")\n",
        "\n",
        "# ----------- Disclaimer - Not mine code (AI generated) -----------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDQpyIPO-vCV"
      },
      "source": [
        "### 5. Apply the augmentation phase using an SLM (could be the one you have used in step 2 or the only aligned with DPO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSrJkh_gFJgy",
        "outputId": "0396dcbe-10f5-46c1-d0fa-a5ccfc15b335"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use the following context to answer the question.\n",
            "Context: in more detail.\n",
            "FIGURE 4. Training Curves for model MSE. (left) 1D Con-\n",
            "volutional Model, (middle) Sequential Model, (right) LSTM\n",
            "model)\n",
            "2. Recurrent Neural Networks\n",
            "A recurrent neural network (RNN) is a model architecture where the output\n",
            "of a hidden node not only depends on the input, but also on the ”past” output\n",
            "from the hidden node. You can therefore think of an RNN as a dynamical\n",
            "system in which the activation levels of the hidden layers are the system’s\n",
            "states. This means that the computation graph of an RNN has self loops in\n",
            "contrast to the graphs of feedforward sequential models.\n",
            "\n",
            "198 6. DEEP LEARNING FOR NATURAL LANGUAGE PROCESSING\n",
            "Let us consider a simple RNN whose output is computed as\n",
            "h(k) = tanh\n",
            "\u0000\n",
            "Ux(k) + Wh(k−1) + b\n",
            "\u0001\n",
            ".(36)\n",
            "In this case we assume that the hidden layer’s activation,s(k) is the model’s\n",
            "output, h(k−1). When we unfold this part of the forward computation graph\n",
            "we can the structure shown in Fig. 6 where the computation in equation (36)\n",
            "is represented by the layer labeled tanh.\n",
            "FIGURE 6. Simple RNN unfolded\n",
            "LSTMs also have the chain like structure shown in Fig. 6. The differ-\n",
            "ence is that the repeating module is more complicated. Instead of having\n",
            "a single neural network layer ( tanh), there are four neural network layers\n",
            "that interact in the ways shown in the Fig. 7. In this figure, each line car-\n",
            "ries an entire vector from the output of one node to the inputs of others.\n",
            "The circles with + or × denote point-wise operations like vector addition\n",
            "and multiplication, respectively. In this figure the yellow boxes represent\n",
            "\n",
            "2. RECURRENT NEURAL NETWORKS 197\n",
            "We can also visualize how an RNN computes and trains byunfolding the\n",
            "computation graph on the left of Fig. 5. This unfolded graph is shown on the\n",
            "right side of Fig. 5. The unfolded graph is obtained by explicitly expand-\n",
            "ing out the delay so we show the computations done at each time step. As\n",
            "seen on the right of Fig. 5, the unfolded graph is, essentially, a feedforward\n",
            "neural network whose weights are shared between all time steps. Fig. 5\n",
            "shows this unfolding for an input sequence of length 3. The graphic explic-\n",
            "itly shows the outputh(k) for each input sequence elementx(k). This output\n",
            "is used to compute the model’s loss, L(y(k), h(k)), with respect to the kth\n",
            "target y(k). This graph represents the forward propagation of the network.\n",
            "We determine the weights, W, V, and U using the backpropagation algo-\n",
            "rithm. Since we have an explicit feedforward acyclic computation graph\n",
            "on the right side of Fig. 5, we can use automatic differentiation to readily\n",
            "Question: what is an RNN?\n",
            "Answer: A recurrent neural network (RNN) is a model architecture where the output of a hidden node not only depends on the input, but also on the “past” output from the hidden node. It can be thought of as a dynamical system in which the activation levels of the hidden layers are the system’s states. This means that the computation graph of an RNN has self-loops in contrast to the graphs of feedforward sequential models.\n"
          ]
        }
      ],
      "source": [
        "final_prompt = f\"\"\"Use the following context to answer the question.\n",
        "Context: {retrieved_context}\n",
        "Question: {paraphrased_questions[0]}\n",
        "Answer:\"\"\"\n",
        "\n",
        "inputs = tokenizer([final_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model_llm.generate(**inputs, max_new_tokens=250)\n",
        "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}