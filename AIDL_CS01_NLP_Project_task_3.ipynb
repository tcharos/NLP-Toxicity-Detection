{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tcharos/NLP-Toxicity-Detection/blob/main/AIDL_CS01_NLP_Project_task_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMDT4GCHCpMd"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install langchain-community langchain-text-splitters pypdf sentence-transformers numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ],
      "metadata": {
        "id": "HVGq3zaTo_XY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch"
      ],
      "metadata": {
        "id": "2oBBU4LkoGmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nx5KfV0S7Nt2"
      },
      "source": [
        "### 1. Load and segment a PDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmcmf_3s7FKy"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_Zy8pNDVAj_"
      },
      "outputs": [],
      "source": [
        "online_document = \"https://academicweb.nd.edu/~lemmon/courses/deep-learning/lecture-book/deep-learning-book-2025.pdf\"\n",
        "\n",
        "loader = PyPDFLoader(online_document) # define loader\n",
        "pages = loader.load() # load\n",
        "\n",
        "print(f\"Loaded {len(pages)} pages\")\n",
        "print(f\"First page preview:\\n{pages[0].page_content[:500]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kj5WRbQcD0rE"
      },
      "outputs": [],
      "source": [
        "# Split into chunks\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=20,\n",
        "    length_function=len\n",
        ") # define splitter with chunk size 1000 and 20% overlap\n",
        "\n",
        "chunks = splitter.split_documents(pages)\n",
        "\n",
        "print(f\"\\nCreated {len(chunks)} chunks\")\n",
        "print(f\"First chunk:\\n{chunks[0].page_content}\")\n",
        "print(f\"Metadata: {chunks[0].metadata}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWF8OFkZ8F1h"
      },
      "source": [
        "### 2. Embed and store in numpy array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqTh0DJoD80f"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "model_emb = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Extract text and generate embeddings\n",
        "texts = [doc.page_content for doc in chunks]\n",
        "embeddings_array = model_emb.encode(texts, convert_to_numpy=True)\n",
        "\n",
        "# Save and reload example\n",
        "np.save('embeddings.npy', embeddings_array)\n",
        "embeddings_array = np.load('embeddings.npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yb9fXTUS8tjn"
      },
      "source": [
        "### 3. Use an S/LLM from Hugginface for paraphrase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7kZRU6-871s"
      },
      "outputs": [],
      "source": [
        "# use a model, check how to do that in HF, for example check this model card https://huggingface.co/google/gemma-2b\n",
        "#  or select on from unsloath\n",
        "# paraphrase 2 times (create the necessary prompt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and tokenizer\n",
        "model_llm, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "FastLanguageModel.for_inference(model_llm)\n",
        "\n",
        "def get_paraphrases(question, n=2):\n",
        "    prompt = f\"Paraphrase the following question in {n} different ways. Return only the paraphrased questions, one per line:\\n\\nQuestion: {question}\"\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model_llm.generate(**inputs, max_new_tokens=100)\n",
        "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "    # Simple split by newline (adjust based on model output)\n",
        "    return [question] + decoded.split('\\n')[-n:]\n",
        "\n",
        "question = \"what is an RNN?\"\n",
        "paraphrased_queries = get_paraphrases(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXqr7VGvCl9t",
        "outputId": "6170d0ff-95f8-44e3-8885-fb8cda146411"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2026.1.2: Fast Llama patching. Transformers: 4.57.3.\n",
            "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.161 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Paraphrased Qustion ---\")\n",
        "for i, query in enumerate(paraphrased_queries, 1):\n",
        "    print(f\"Paraphrase {i}: {query}\")"
      ],
      "metadata": {
        "id": "1h7MBjRRnMMo",
        "outputId": "4a330615-adcc-42ea-cfd9-53b82742bcb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Paraphrased Qustion ---\n",
            "Paraphrase 1: what is an RNN?\n",
            "Paraphrase 2: 1. What is the term for a particular kind of recurrent neural network that is often used in machine learning applications?\n",
            "Paraphrase 3: 2. What is a type of neural network design that is optimized for handling sequential data?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L00SX0vG-f1m"
      },
      "source": [
        "### 4. Retrieve 5 most semantically close chunks (cosine similarity) for every paraphrase*, then add threshold 0.3 and select top 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MI38y5WpEqfp"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "all_indices = []\n",
        "threshold = 0.3\n",
        "\n",
        "for q in paraphrased_queries:\n",
        "    q_emb = model_emb.encode([q])\n",
        "    sims = cosine_similarity(q_emb, embeddings_array)[0]\n",
        "\n",
        "    # Get indices of top 5 that also meet threshold\n",
        "    top_5_idx = sims.argsort()[-5:][::-1]\n",
        "    filtered_idx = [i for i in top_5_idx if sims[i] >= threshold]\n",
        "    all_indices.extend(filtered_idx)\n",
        "\n",
        "# Unique indices and select final top 3 (simplification: take the first 3 unique)\n",
        "unique_top_indices = list(dict.fromkeys(all_indices))[:3]\n",
        "retrieved_context = \"\\n\".join([chunks[i].page_content for i in unique_top_indices])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# 1. Combine original question and paraphrases for a broader search\n",
        "all_queries = [question] + paraphrased_queries\n",
        "candidate_pool = []\n",
        "threshold = 0.3\n",
        "\n",
        "print(f\"--- Verification: Searching for {len(all_queries)} query variations ---\")\n",
        "\n",
        "for q in all_queries:\n",
        "    # Embed the specific query\n",
        "    q_emb = model_emb.encode([q])\n",
        "\n",
        "    # Calculate cosine similarity against all chunk embeddings\n",
        "    sims = cosine_similarity(q_emb, embeddings_array)[0]\n",
        "\n",
        "    # Get indices of top 5 matches\n",
        "    top_5_idx = sims.argsort()[-5:][::-1]\n",
        "\n",
        "    # Filter by threshold and print findings for verification\n",
        "    for idx in top_5_idx:\n",
        "        score = sims[idx]\n",
        "        if score >= threshold:\n",
        "            candidate_pool.append((idx, score))\n",
        "            print(f\"Match Found! Query: '{q[:30]}...' | Score: {score:.4f} | Index: {idx}\")\n",
        "\n",
        "# 2. Sort all matches by score and remove duplicates\n",
        "candidate_pool.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "final_indices = []\n",
        "seen_indices = set()\n",
        "\n",
        "for idx, score in candidate_pool:\n",
        "    if idx not in seen_indices:\n",
        "        final_indices.append(idx)\n",
        "        seen_indices.add(idx)\n",
        "    if len(final_indices) == 3: # Limit to top 3 as per requirements\n",
        "        break\n",
        "\n",
        "# 3. Final Verification Print\n",
        "print(f\"\\n--- Final Selection (Top {len(final_indices)} Unique Chunks) ---\")\n",
        "for i, idx in enumerate(final_indices):\n",
        "    content_preview = chunks[idx].page_content[:150].replace('\\n', ' ')\n",
        "    print(f\"{i+1}. [Index {idx}] Content: {content_preview}...\")\n",
        "\n",
        "# Store for Step 5\n",
        "retrieved_context = \"\\n\\n\".join([chunks[i].page_content for i in final_indices])"
      ],
      "metadata": {
        "id": "IyLRUrfNnjhf",
        "outputId": "9f98c09a-069f-4198-9a62-cfaa705b1990",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Verification: Searching for 4 query variations ---\n",
            "Match Found! Query: 'what is an RNN?...' | Score: 0.6279 | Index: 403\n",
            "Match Found! Query: 'what is an RNN?...' | Score: 0.4881 | Index: 407\n",
            "Match Found! Query: 'what is an RNN?...' | Score: 0.4469 | Index: 487\n",
            "Match Found! Query: 'what is an RNN?...' | Score: 0.4154 | Index: 485\n",
            "Match Found! Query: 'what is an RNN?...' | Score: 0.4137 | Index: 416\n",
            "Match Found! Query: 'what is an RNN?...' | Score: 0.6279 | Index: 403\n",
            "Match Found! Query: 'what is an RNN?...' | Score: 0.4881 | Index: 407\n",
            "Match Found! Query: 'what is an RNN?...' | Score: 0.4469 | Index: 487\n",
            "Match Found! Query: 'what is an RNN?...' | Score: 0.4154 | Index: 485\n",
            "Match Found! Query: 'what is an RNN?...' | Score: 0.4137 | Index: 416\n",
            "Match Found! Query: '1. What is the term for a part...' | Score: 0.6313 | Index: 403\n",
            "Match Found! Query: '1. What is the term for a part...' | Score: 0.5886 | Index: 416\n",
            "Match Found! Query: '1. What is the term for a part...' | Score: 0.5875 | Index: 407\n",
            "Match Found! Query: '1. What is the term for a part...' | Score: 0.5602 | Index: 406\n",
            "Match Found! Query: '1. What is the term for a part...' | Score: 0.5511 | Index: 390\n",
            "Match Found! Query: '2. What is a type of neural ne...' | Score: 0.5278 | Index: 407\n",
            "Match Found! Query: '2. What is a type of neural ne...' | Score: 0.5256 | Index: 403\n",
            "Match Found! Query: '2. What is a type of neural ne...' | Score: 0.4832 | Index: 291\n",
            "Match Found! Query: '2. What is a type of neural ne...' | Score: 0.4797 | Index: 137\n",
            "Match Found! Query: '2. What is a type of neural ne...' | Score: 0.4769 | Index: 191\n",
            "\n",
            "--- Final Selection (Top 3 Unique Chunks) ---\n",
            "1. [Index 403] Content: in more detail. FIGURE 4. Training Curves for model MSE. (left) 1D Con- volutional Model, (middle) Sequential Model, (right) LSTM model) 2. Recurrent ...\n",
            "2. [Index 416] Content: 202 6. DEEP LEARNING FOR NATURAL LANGUAGE PROCESSING There are two useful variations on the LSTM that are frequently used; gated recurrent units (GRU)...\n",
            "3. [Index 407] Content: compute the gradient and perform a stochastic gradient descent algorithm. This approach to training an RNN is often called Backpropagation through tim...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-ranking (not mandatory)\n",
        "\n",
        "# ----------- Disclaimer - Not mine code (AI generated) but I wanted to try it out -----------\n",
        "\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "rerank_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "original_query = all_queries[0]\n",
        "pairs = [[original_query, chunk] for chunk in retrieved_context]\n",
        "\n",
        "rerank_scores = rerank_model.predict(pairs)\n",
        "\n",
        "reranked_results = sorted(\n",
        "    zip(retrieved_context, rerank_scores),\n",
        "    key=lambda x: x[1],\n",
        "    reverse=True\n",
        ")\n",
        "\n",
        "final_retrieved_context = [chunk for chunk, score in reranked_results[:3]]\n",
        "\n",
        "print(\"Re-ranking complete. Chunks are now ordered by relevance to the original question.\")\n",
        "\n",
        "# ----------- Disclaimer - Not mine code (AI generated) -----------"
      ],
      "metadata": {
        "id": "uuPR9KABxyRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDQpyIPO-vCV"
      },
      "source": [
        "### 5. Apply the augmentation phase using an SLM (could be the one you have used in step 2 or the only aligned with DPO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "WSrJkh_gFJgy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd5fc168-6b9e-4c5c-db43-dbae2f7ac86d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use the following context to answer the question.\n",
            "Context: in more detail.\n",
            "FIGURE 4. Training Curves for model MSE. (left) 1D Con-\n",
            "volutional Model, (middle) Sequential Model, (right) LSTM\n",
            "model)\n",
            "2. Recurrent Neural Networks\n",
            "A recurrent neural network (RNN) is a model architecture where the output\n",
            "of a hidden node not only depends on the input, but also on the ”past” output\n",
            "from the hidden node. You can therefore think of an RNN as a dynamical\n",
            "system in which the activation levels of the hidden layers are the system’s\n",
            "states. This means that the computation graph of an RNN has self loops in\n",
            "contrast to the graphs of feedforward sequential models.\n",
            "\n",
            "202 6. DEEP LEARNING FOR NATURAL LANGUAGE PROCESSING\n",
            "There are two useful variations on the LSTM that are frequently used;\n",
            "gated recurrent units (GRU) and bidirectional LSTMs. A GRU [ CGCB14,\n",
            "CVMG+14] may be seen as a simplified version an LSTM whose operation\n",
            "can again be explained in terms of ”gating layers”. Keras also has a GRU\n",
            "layer whose API is similar to that of the LSTM layer.\n",
            "The second type of LSTM is a bidirectional RNN. Note that simple\n",
            "RNN’s and LSTM process inputs in an order-dependent manner. In other\n",
            "words, these networks are ”causal” in the sense that the future outputs\n",
            "are determined solely by the past outputs. There are problems, however,\n",
            "where the future inputs also influence earlier outputs. This is particularly\n",
            "true in text and language processing applications [GFS05]. A bidirectional\n",
            "RNN uses two regular RNNs, one of which processes inputs in the forward\n",
            "(causal) direction and the other processing inputs in the reverse (anti-causal)\n",
            "\n",
            "compute the gradient and perform a stochastic gradient descent algorithm.\n",
            "This approach to training an RNN is often called Backpropagation through\n",
            "time or BPTT [Wer90].\n",
            "Note that the run time for the update isO(N) where N is the length of the\n",
            "input sequence. This, unfortunately, cannot be reduced through paralleliza-\n",
            "tion because of the sequential nature of the forward computation graph.\n",
            "Each time step can only be computed after the previous one was computed.\n",
            "As a result the state computed in the forward pass must be stored until it can\n",
            "be used in the backward training pass and so the memory cost of training an\n",
            "RNN is O(N). This means that RNN’s will require more computational re-\n",
            "sources (memory) for training than a similarly sized sequential feedforward\n",
            "network.\n",
            "2.1. LSTM Recurrent Networks. Long Short Term memory networks\n",
            "(LSTM) [HS97] are a special kind of RNN that have been refined and popu-\n",
            "larized. LSTMs were routinely used in time-series prediction and language\n",
            "Question: what is an RNN?\n",
            "Answer: A recurrent neural network (RNN) is a model architecture where the output of a hidden node not only depends on the input, but also on the \"past\" output from the hidden node. An RNN can therefore be thought of as a dynamical system in which the activation levels of the hidden layers are the system's states. This means that the computation graph of an RNN has self-loops in contrast to the graphs of feedforward sequential models. (Source: Recurrent Neural Networks)\n"
          ]
        }
      ],
      "source": [
        "final_prompt = f\"\"\"Use the following context to answer the question.\n",
        "Context: {retrieved_context}\n",
        "Question: {paraphrased_queries[0]}\n",
        "Answer:\"\"\"\n",
        "\n",
        "inputs = tokenizer([final_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model_llm.generate(**inputs, max_new_tokens=250)\n",
        "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}