{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIDL_B_CS01: Advanced NLP Project\n",
    "\n",
    "1. LSTM Toxicity Detection (Custom & GloVe Embeddings)\n",
    "2. BERT-alike STS-b Semantic Similarity (PyTorch/HF)\n",
    "3. Custom RAG Mechanism\n",
    "5. LLM Tuning with DPO (Gordon Ramsay Alignment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc91b44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# ==========================================================\n",
    "# 1. Environment Detection & Constants\n",
    "# ==========================================================\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "BASE_DIR = \"/content\" if IN_COLAB else \".\"\n",
    "TOXICITY_PATH = os.path.join(BASE_DIR, \"data_sets/toxicity\")\n",
    "\n",
    "# ==========================================================\n",
    "# 2. Dependency Installation\n",
    "# ==========================================================\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab. Installing NLP stack...\")\n",
    "    !pip install -q -U transformers datasets accelerate peft trl sentence-transformers\n",
    "else:\n",
    "    print(\"Running locally. Checking Mac-specific requirements...\")\n",
    "    !{sys.executable} -m pip install -q \"tensorflow==2.16.2\" \"tensorflow-macos==2.16.2\" \"tf-keras~=2.16\"\n",
    "    !{sys.executable} -m pip install -q -U transformers datasets accelerate peft trl sentence-transformers\n",
    "\n",
    "# ==========================================================\n",
    "# 3. Keras Backend Configuration (CRITICAL: Must be before Keras imports)\n",
    "# ==========================================================\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "# ==========================================================\n",
    "# 4. Comprehensive Imports\n",
    "# ==========================================================\n",
    "# --- Core Libraries ---\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Metrics ---\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "# --- PyTorch/Transformers/Datasets ---\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoModelForCausalLM, \n",
    "    Trainer, \n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "# --- DPO/PEFT (Task 5) ---\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import DPOTrainer\n",
    "\n",
    "# --- Keras/TensorFlow Layers (Task 1) ---\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# ==========================================================\n",
    "# 5. Hardware Verification\n",
    "# ==========================================================\n",
    "print(f\"\\nTensorFlow Version: {tf.__version__}\")\n",
    "# Check for Apple Silicon (Metal) or CUDA\n",
    "print(\"Num GPUs Available (TF): \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"PyTorch Device: Mac GPU (Metal)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"PyTorch Device: Colab GPU (CUDA)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"PyTorch Device: CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc76ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(TOXICITY_PATH, exist_ok=True)\n",
    "\n",
    "def download_toxicity_data():\n",
    "    urls = {\n",
    "        \"train.csv\": \"https://raw.githubusercontent.com/Sreyan88/Toxicity-Detection-in-Spoken-Utterances/main/data/train.csv\",\n",
    "        \"valid.csv\": \"https://raw.githubusercontent.com/Sreyan88/Toxicity-Detection-in-Spoken-Utterances/main/data/valid.csv\",\n",
    "        \"test.csv\": \"https://raw.githubusercontent.com/Sreyan88/Toxicity-Detection-in-Spoken-Utterances/main/data/test.csv\"\n",
    "    }\n",
    "    for name, url in urls.items():\n",
    "        local_file = os.path.join(TOXICITY_PATH, name)\n",
    "        if not os.path.exists(local_file):\n",
    "            print(f\"Fetching {name} from GitHub...\")\n",
    "            tf.keras.utils.get_file(os.path.abspath(local_file), origin=url)\n",
    "\n",
    "download_toxicity_data()\n",
    "print(\"\\nInitialization Complete. All systems ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814025e8",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aeec35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_with_encoding(filename):\n",
    "    try:\n",
    "        return pd.read_csv(filename, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"UTF-8 failed for {filename}, falling back to latin1\")\n",
    "        return pd.read_csv(filename, encoding='latin1')\n",
    "\n",
    "def evaluate_binary(y_true, y_pred_probs, model_name=\"LSTM Model\"):\n",
    "    \"\"\"\n",
    "    Evaluates Task 1: Toxicity Detection.\n",
    "    Expects y_pred_probs as probabilities (0 to 1).\n",
    "    \"\"\"\n",
    "    # Convert probabilities to binary classes\n",
    "    y_pred = (np.array(y_pred_probs) > 0.5).astype(int)\n",
    "    \n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\n--- Binary Classification: {model_name} ---\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Clean', 'Toxic'], yticklabels=['Clean', 'Toxic'])\n",
    "    plt.title(f'Toxicity Confusion Matrix\\n{model_name}')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()\n",
    "\n",
    "    return f1, cm\n",
    "\n",
    "def evaluate_stsb(y_true, y_pred, model_name=\"BERT Model\"):\n",
    "    \"\"\"\n",
    "    Evaluates Task 2: STS-b Semantic Similarity.\n",
    "    Expects y_pred as float regression scores.\n",
    "    \"\"\"\n",
    "    # 1. Compute Official Metrics (Pearson & Spearman)\n",
    "    pearson_val, _ = pearsonr(y_true, y_pred)\n",
    "    spearman_val, _ = spearmanr(y_true, y_pred)\n",
    "    \n",
    "    # 2. Prepare Confusion Matrix (Discretize 0-5)\n",
    "    y_true_int = np.rint(y_true).astype(int)\n",
    "    y_pred_int = np.clip(np.rint(y_pred), 0, 5).astype(int)\n",
    "    cm = confusion_matrix(y_true_int, y_pred_int)\n",
    "    \n",
    "    print(f\"\\n--- STS-b Regression: {model_name} ---\")\n",
    "    print(f\"Pearson Correlation:  {pearson_val:.4f}\")\n",
    "    print(f\"Spearman Correlation: {spearman_val:.4f}\")\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens')\n",
    "    plt.title(f'STS-b Quantized Confusion Matrix\\n{model_name}')\n",
    "    plt.ylabel('Actual (Rounded)')\n",
    "    plt.xlabel('Predicted (Rounded)')\n",
    "    plt.show()\n",
    "\n",
    "    return pearson_val, spearman_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Toxicity Detection with LSTM (Custom & GloVe Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.1. Data Loading and Preparation\n",
    "\n",
    "DATA_PATH = os.path.join(BASE_DIR, \"data_sets/toxicity/\")\n",
    "\n",
    "train_df = load_with_encoding(os.path.join(DATA_PATH, \"train.csv\"))\n",
    "valid_df = load_with_encoding(os.path.join(DATA_PATH, \"valid.csv\"))\n",
    "test_df = load_with_encoding(os.path.join(DATA_PATH, \"test.csv\"))\n",
    "\n",
    "X_train, y_train = train_df['text'].astype(str), train_df['label2a']\n",
    "X_valid, y_valid = valid_df['text'].astype(str), valid_df['label2a']\n",
    "X_test, y_test = test_df['text'].astype(str), test_df['label2a']\n",
    "\n",
    "MAX_WORDS = 10000 \n",
    "MAX_LEN = 60\n",
    "EMBEDDING_DIM = 100 \n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_pad = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=MAX_LEN)\n",
    "X_valid_pad = pad_sequences(tokenizer.texts_to_sequences(X_valid), maxlen=MAX_LEN)\n",
    "X_test_pad = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=MAX_LEN)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e018496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(embedding_layer):\n",
    "    model = Sequential([\n",
    "        embedding_layer,\n",
    "        SpatialDropout1D(0.2),\n",
    "        # On Mac, avoid recurrent_dropout=0.2 for significantly faster GPU execution\n",
    "        LSTM(64, dropout=0.2), \n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.2. Model 1: LSTM with Custom Learned Embeddings\n",
    "\n",
    "# --- PART A: CUSTOM EMBEDDINGS ---\n",
    "print(\"Training LSTM with Custom Embeddings...\")\n",
    "custom_emb_layer = Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_LEN)\n",
    "model_custom = build_lstm(custom_emb_layer)\n",
    "\n",
    "model_custom.fit(X_train_pad, y_train, validation_data=(X_valid_pad, y_valid), epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc68aea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_probs = model_custom.predict(X_test_pad).flatten()\n",
    "\n",
    "evaluate_binary(y_test, custom_probs, model_name=\"LSTM (Custom Embeddings)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PART B: GLOVE EMBEDDINGS ---\n",
    "def get_glove_matrix(path, word_index, emb_dim):\n",
    "    embeddings_index = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "            \n",
    "    matrix = np.zeros((len(word_index) + 1, emb_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            matrix[i] = embedding_vector\n",
    "    return matrix\n",
    "\n",
    "# ACTION: Update this path to your local glove file\n",
    "GLOVE_PATH = os.path.join(BASE_DIR, \"glove.6B.100d.txt\")\n",
    "\n",
    "if not os.path.exists(GLOVE_PATH):\n",
    "    if IN_COLAB:\n",
    "        print(\"Downloading GloVe for Colab...\")\n",
    "        !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "        !unzip -j glove.6B.zip \"glove.6B.100d.txt\" -d {BASE_DIR}\n",
    "    else:\n",
    "        print(f\"Please ensure GloVe is at {GLOVE_PATH}\")\n",
    "\n",
    "try:\n",
    "    embedding_matrix = get_glove_matrix(GLOVE_PATH, tokenizer.word_index, EMBEDDING_DIM)\n",
    "    glove_emb_layer = Embedding(vocab_size, EMBEDDING_DIM, weights=[embedding_matrix], \n",
    "                                input_length=MAX_LEN, trainable=False)\n",
    "    \n",
    "    print(\"\\nTraining LSTM with GloVe Embeddings...\")\n",
    "    model_glove = build_lstm(glove_emb_layer)\n",
    "    model_glove.fit(X_train_pad, y_train, validation_data=(X_valid_pad, y_valid), epochs=5, batch_size=32)\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nSkipping GloVe: Could not find {GLOVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89b9797",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_probs = model_glove.predict(X_test_pad)\n",
    "\n",
    "evaluate_binary(y_test, glove_probs, model_name=\"LSTM (GloVe Embeddings)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Semantic Similarity (STS-b) with BERT-alike Models (PyTorch/HF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_files = {\n",
    "#     \"train\": \"./data_sets/sts_b/train.jsonl\",\n",
    "#     \"validation\": \"./data_sets/sts_b/validation.jsonl\",\n",
    "#     \"test\": \"./data_sets/sts_b/test.jsonl\"\n",
    "# }\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"stsb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf9cd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sts(examples, tokenizer):\n",
    "    # STS-b expects two sentences and a float label\n",
    "    return tokenizer(\n",
    "        examples[\"sentence1\"], \n",
    "        examples[\"sentence2\"], \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "def train_transformer(model_id, label_name=\"BERT\"):\n",
    "    print(f\"\\n--- Training {label_name} ({model_id}) ---\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    \n",
    "    tokenized_datasets = raw_datasets.map(lambda x: preprocess_sts(x, tokenizer), batched=True)\n",
    "    tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=1)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results_{label_name.lower()}\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        # Only use FP16 on Colab (CUDA); Mac (MPS) prefers FP32 or BF16\n",
    "        fp16=HAS_CUDA, \n",
    "        report_to=\"none\" \n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    return trainer, tokenized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5e8690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: BERT\n",
    "trainer_bert, test_data_bert = train_transformer(\"bert-base-uncased\", \"BERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc094e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_results = trainer_bert.predict(test_data_bert)\n",
    "preds_bert = bert_results.predictions.flatten()\n",
    "\n",
    "true_labels_bert = test_data_bert[\"labels\"]\n",
    "\n",
    "evaluate_stsb(true_labels_bert, preds_bert, model_name=\"BERT-Base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795e6384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: RoBERTa\n",
    "trainer_roberta, test_data_roberta = train_transformer(\"roberta-base\", \"RoBERTa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91241dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_results = trainer_roberta.predict(test_data_roberta)\n",
    "preds_roberta = roberta_results.predictions.flatten()\n",
    "\n",
    "true_labels_roberta = test_data_roberta[\"labels\"]\n",
    "\n",
    "evaluate_stsb(true_labels_roberta, preds_roberta, model_name=\"RoBERTa-Base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Custom RAG (Retrieval-Augmented Generation) Mechanism\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.1. Knowledge Base Preparation\n",
    "\n",
    "# **ACTION REQUIRED: Define or Load Knowledge Base**\n",
    "KB_DOCS = [\n",
    "    \"Doc 1: Transformer architecture uses multi-head attention to weigh the importance of different words in the input sequence. This allows parallelization.\",\n",
    "    \"Doc 2: Recurrent Neural Networks (RNNs) suffer from the vanishing gradient problem, which LSTMs and GRUs were designed to solve through gating mechanisms.\",\n",
    "    # Add your documents here\n",
    "]\n",
    "\n",
    "# **ACTION REQUIRED: Chunking and Embedding**\n",
    "# 1. Chunk documents (e.g., using a text splitter).\n",
    "# 2. Embed the chunks (e.g., using 'sentence-transformers/all-MiniLM-L6-v2').\n",
    "# 3. Store chunks and embeddings in a Vector Store (e.g., simple list, Faiss, or Chroma).\n",
    "\n",
    "# Example Placeholder for a simple in-memory store (requires implementation of actual embedding)\n",
    "# class SimpleVectorStore:\n",
    "#     def __init__(self, docs, embed_model):\n",
    "#         self.chunks = docs\n",
    "#         self.embeddings = [embed_model.encode(c) for c in docs]\n",
    "#         self.embed_model = embed_model\n",
    "#     def search(self, query, k=3):\n",
    "#         query_embedding = self.embed_model.encode(query)\n",
    "#         # Implement similarity calculation (e.g., cosine similarity) and return top k chunks\n",
    "#         return self.chunks[:k] # Placeholder return\n",
    "# embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# vector_store = SimpleVectorStore(KB_DOCS, embed_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.2. Retrieval and Generation Logic\n",
    "\n",
    "# **ACTION REQUIRED: Define your LLM for generation**\n",
    "# llm_pipeline = pipeline('text-generation', model='your-small-llm-model')\n",
    "\n",
    "def custom_rag_mechanism(query, top_k=3):\n",
    "    \"\"\"Performs retrieval and augmented generation.\"\"\"\n",
    "    \n",
    "    # --- Step 1: Retrieval (Requires `vector_store` from above) ---\n",
    "    # retrieved_chunks = vector_store.search(query, k=top_k)\n",
    "    retrieved_chunks = [\"<retrieved_chunk_1>\", \"<retrieved_chunk_2>\"] # TEMPORARY\n",
    "    \n",
    "    # --- Step 2: Context Formatting ---\n",
    "    context = \"\\n\\n\".join(retrieved_chunks)\n",
    "    \n",
    "    # --- Step 3: Generation (LLM Call) ---\n",
    "    LLM_PROMPT = f\"\"\"\n",
    "    You are an AI assistant. Use the following CONTEXT to answer the USER QUESTION. \n",
    "    If the CONTEXT does not contain the answer, state that you cannot answer based on the provided information.\n",
    "\n",
    "    CONTEXT:\n",
    "    {context}\n",
    "\n",
    "    USER QUESTION: {query}\n",
    "\n",
    "    ANSWER:\n",
    "    \"\"\"\n",
    "    \n",
    "    # response = llm_pipeline(LLM_PROMPT, max_new_tokens=100)[0]['generated_text']\n",
    "    response = \"RAG Answer based on the retrieved context!\"\n",
    "    return response, retrieved_chunks\n",
    "\n",
    "# Example Test\n",
    "user_query = \"What were LSTMs designed to solve in traditional RNNs?\"\n",
    "answer, context_used = custom_rag_mechanism(user_query)\n",
    "print(f\"Query: {user_query}\")\n",
    "print(f\"Answer: {answer}\")\n",
    "print(f\"Context Used: {context_used}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LLM Tuning with DPO for Gordon Ramsay Persona\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5.1. Dataset Preparation\n",
    "\n",
    "# **ACTION REQUIRED: Update file path**\n",
    "PREFERENCE_DATA_PATH = \"path/to/task4_ramsay_preference_dataset.csv\" \n",
    "\n",
    "# Load the dataset created in Task 4\n",
    "# DPO_DF = pd.read_csv(PREFERENCE_DATA_PATH)\n",
    "\n",
    "# DPO requires (prompt, chosen_response, rejected_response)\n",
    "# dpo_dataset_df = DPO_DF.rename(columns={\n",
    "#     'Question': 'prompt',\n",
    "#     'Ramsay': 'chosen', # This is the preferred answer\n",
    "#     'Polite': 'rejected' # This is the non-preferred answer\n",
    "# })\n",
    "\n",
    "# Convert to Hugging Face Dataset format (requires `from datasets import Dataset`)\n",
    "# dpo_hf_dataset = Dataset.from_pandas(dpo_dataset_df[['prompt', 'chosen', 'rejected']])\n",
    "# dpo_hf_dataset = dpo_hf_dataset.train_test_split(test_size=0.1) # Split for validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5.2. Model and DPO Trainer Setup\n",
    "\n",
    "# **ACTION REQUIRED: Choose your base model**\n",
    "BASE_MODEL = \"facebook/opt-125m\" # Use a small model for development, or a larger one if resources allow\n",
    "\n",
    "# 1. Load the base model and tokenizer\n",
    "# model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, torch_dtype=torch.bfloat16) # Use a suitable dtype\n",
    "# tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "# tokenizer.pad_token = tokenizer.eos_token # Ensure pad token is set\n",
    "\n",
    "# 2. Setup PEFT/LoRA configuration\n",
    "# peft_config = LoraConfig(\n",
    "#     r=16,\n",
    "#     lora_alpha=16,\n",
    "#     target_modules=[\"q_proj\", \"v_proj\"], # Check model documentation for correct layers\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "# )\n",
    "\n",
    "# 3. DPO Training Arguments\n",
    "# training_args_dpo = TrainingArguments(\n",
    "#     output_dir=\"./dpo_results_ramsay\",\n",
    "#     num_train_epochs=1,\n",
    "#     per_device_train_batch_size=4,\n",
    "#     gradient_accumulation_steps=4,\n",
    "#     logging_steps=10,\n",
    "#     learning_rate=5e-5,\n",
    "#     remove_unused_columns=False,\n",
    "#     save_strategy=\"epoch\",\n",
    "#     fp16=True, # Use fp16/bf16 if supported\n",
    "#     report_to=\"none\"\n",
    "# )\n",
    "\n",
    "# 4. Initialize DPOTrainer\n",
    "# dpo_trainer = DPOTrainer(\n",
    "#     model=model,\n",
    "#     ref_model=None, # Set to None for implicit reference model loading\n",
    "#     args=training_args_dpo,\n",
    "#     beta=0.1, \n",
    "#     train_dataset=dpo_hf_dataset['train'],\n",
    "#     eval_dataset=dpo_hf_dataset['test'],\n",
    "#     tokenizer=tokenizer,\n",
    "#     peft_config=peft_config,\n",
    "# )\n",
    "\n",
    "# 5. Train Placeholder\n",
    "# print(\"Starting DPO Training...\")\n",
    "# dpo_trainer.train()\n",
    "\n",
    "# 6. Save the final model (LoRA weights)\n",
    "# dpo_trainer.save_model(\"ramsay_dpo_adapter\")\n",
    "\n",
    "# **ACTION REQUIRED: Inference Test**\n",
    "# Test the fine-tuned model with a new question to verify the Ramsay persona.\n",
    "# from peft import PeftModel\n",
    "# ft_model = PeftModel.from_pretrained(model, \"ramsay_dpo_adapter\")\n",
    "# ft_model.eval()\n",
    "# print(\"\\n--- DPO Fine-Tuned Model Test ---\")\n",
    "# test_prompt = \"Why is batch normalization useful?\"\n",
    "# ... generate response ...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_msc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
