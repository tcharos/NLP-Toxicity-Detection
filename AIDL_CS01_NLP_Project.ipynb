{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# AIDL_B_CS01: Advanced NLP Project\n\n**Tasks Covered:**\n1. LSTM Toxicity Detection (Custom & GloVe Embeddings)\n2. BERT-alike STS-b Semantic Similarity (PyTorch/HF)\n3. Custom RAG Mechanism\n5. LLM Tuning with DPO (Gordon Ramsay Alignment)\n\n*(Task 4, the dataset creation, is a prerequisite and not coded here.)*"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## ðŸ’» Setup and Imports\n\n# Install necessary libraries (uncomment if needed)\n# !pip install tensorflow keras torch transformers datasets scikit-learn pandas numpy scipy trl peft sentence-transformers\n\n# --- Core Libraries ---\nimport pandas as pd\nimport numpy as np\nimport os\n\n# --- Metrics ---\nfrom scipy.stats import pearsonr, spearmanr # Task 2 correlation metrics\nfrom sklearn.metrics import f1_score, confusion_matrix\n\n# --- TensorFlow/Keras (Task 1) ---\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# --- PyTorch/Transformers/Datasets (Task 2 & 5) ---\nimport torch\nfrom datasets import load_dataset, Dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM, Trainer, TrainingArguments\n\n# --- DPO/PEFT (Task 5) ---\nfrom peft import LoraConfig, get_peft_model\nfrom trl import DPOTrainer\n\n# --- RAG (Task 3) ---\n# Specific imports depend on your chosen vector store and embedding method\n# from sentence_transformers import SentenceTransformer \n# from langchain.text_splitter import RecursiveCharacterTextSplitter # Example chunking tool\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Toxicity Detection with LSTM (Custom & GloVe Embeddings)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### 1.1. Data Loading and Preparation\n\n# **ACTION REQUIRED: Update file paths**\nTRAIN_PATH = \"data/train.csv\"\nVALID_PATH = \"data/valid.csv\"\nTEST_PATH = \"data/test.csv\"\n\n# Load Data\ndf_train = pd.read_csv(TRAIN_PATH)\ndf_valid = pd.read_csv(VALID_PATH)\ndf_test = pd.read_csv(TEST_PATH)\n\n# Separate features (text) and labels (Toxicity)\nX_train, y_train = df_train['Utterance'], df_train['Toxicity']\nX_valid, y_valid = df_valid['Utterance'], df_valid['Toxicity']\nX_test, y_test = df_test['Utterance'], df_test['Toxicity']\n\n# Configuration\nMAX_WORDS = 20000 \nMAX_LEN = 100    \nEMBEDDING_DIM = 100 \n\n# Tokenizer\ntokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(X_train.astype(str))\nvocab_size = len(tokenizer.word_index) + 1\n\n# Convert texts to sequences and pad them\nX_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train.astype(str)), maxlen=MAX_LEN, padding='post', truncating='post')\nX_valid_seq = pad_sequences(tokenizer.texts_to_sequences(X_valid.astype(str)), maxlen=MAX_LEN, padding='post', truncating='post')\nX_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test.astype(str)), maxlen=MAX_LEN, padding='post', truncating='post')\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### 1.2. Model 1: LSTM with Custom Learned Embeddings\n\ndef create_custom_lstm_model(vocab_size, embedding_dim, max_len):\n    \"\"\"Defines an LSTM model with an initialized Embedding layer.\"\"\"\n    model = Sequential([\n        Embedding(vocab_size, embedding_dim, input_length=max_len),\n        SpatialDropout1D(0.2),\n        LSTM(100, dropout=0.2, recurrent_dropout=0.2),\n        Dense(1, activation='sigmoid') \n    ])\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\nmodel_custom = create_custom_lstm_model(vocab_size, EMBEDDING_DIM, MAX_LEN)\nmodel_custom.summary()\n\n# **ACTION REQUIRED: Training**\n# history_custom = model_custom.fit(\n#     X_train_seq, y_train,\n#     epochs=10,\n#     batch_size=32,\n#     validation_data=(X_valid_seq, y_valid)\n# )\n\n# **ACTION REQUIRED: Evaluation**\n# y_pred_custom_proba = model_custom.predict(X_test_seq)\n# y_pred_custom = (y_pred_custom_proba > 0.5).astype(int)\n\n# f1_custom = f1_score(y_test, y_pred_custom)\n# cm_custom = confusion_matrix(y_test, y_pred_custom)\n# print(f\"\\n--- Custom Embeddings Results ---\")\n# print(f\"F1 Score: {f1_custom:.4f}\")\n# print(\"Confusion Matrix:\\n\", cm_custom)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### 1.3. Model 2: LSTM with GloVe Pre-trained Embeddings\n\n# **ACTION REQUIRED: GloVe Loading and Embedding Matrix Creation**\n# 1. Load the GloVe file (e.g., 'glove.6B.100d.txt')\n# 2. Parse the vectors into an index map.\n# 3. Create the embedding_matrix for your vocabulary.\n\n# Placeholder for embedding_matrix (replace with actual loading logic)\n# embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n# for word, i in tokenizer.word_index.items():\n#     if i < vocab_size:\n#         # embedding_vector = embeddings_index.get(word)\n#         # if embedding_vector is not None:\n#         #     embedding_matrix[i] = embedding_vector\n#         pass # Temporary placeholder\n\n# NOTE: Run the cell above and verify embedding_matrix is correctly built before running this cell\n\n# def create_glove_lstm_model(embedding_matrix, max_len):\n#     \"\"\"Defines an LSTM model using pre-trained GloVe weights.\"\"\"\n#     model = Sequential([\n#         Embedding(\n#             input_dim=embedding_matrix.shape[0],\n#             output_dim=embedding_matrix.shape[1],\n#             weights=[embedding_matrix],\n#             input_length=max_len,\n#             trainable=False # Crucial for pre-trained embeddings\n#         ),\n#         SpatialDropout1D(0.2),\n#         LSTM(100, dropout=0.2, recurrent_dropout=0.2),\n#         Dense(1, activation='sigmoid')\n#     ])\n#     model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n#     return model\n\n# **ACTION REQUIRED: Uncomment and use the function**\n# model_glove = create_glove_lstm_model(embedding_matrix, MAX_LEN)\n# model_glove.summary()\n\n# **ACTION REQUIRED: Training**\n# history_glove = model_glove.fit(...)\n\n# **ACTION REQUIRED: Evaluation**\n# y_pred_glove_proba = model_glove.predict(X_test_seq)\n# y_pred_glove = (y_pred_glove_proba > 0.5).astype(int)\n\n# f1_glove = f1_score(y_test, y_pred_glove)\n# cm_glove = confusion_matrix(y_test, y_pred_glove)\n# print(f\"\\n--- GloVe Embeddings Results ---\")\n# print(f\"F1 Score: {f1_glove:.4f}\")\n# print(\"Confusion Matrix:\\n\", cm_glove)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Semantic Similarity (STS-b) with BERT-alike Models (PyTorch/HF)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### 2.1. Data Loading, Preprocessing, and Metrics\n\nMODEL_NAME_1 = \"bert-base-uncased\" \nMODEL_NAME_2 = \"roberta-base\"      \n\n# Load Dataset\ndataset = load_dataset(\"glue\", \"stsb\")\n\n# Use tokenizer for first model (works for both BERT/RoBERTa base)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_1)\n\n# Preprocessing function for STS-b\ndef preprocess_function(examples):\n    return tokenizer(examples['sentence1'], examples['sentence2'], truncation=True, padding=\"max_length\")\n\ntokenized_datasets = dataset.map(preprocess_function, batched=True)\n\n# Prepare labels (must be float for regression)\ntokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\ntokenized_datasets = tokenized_datasets.map(lambda e: {'labels': float(e['labels'])}, batched=True)\n\n# Select and format splits\ntrain_dataset = tokenized_datasets[\"train\"].remove_columns(['sentence1', 'sentence2', 'idx'])\nvalid_dataset = tokenized_datasets[\"validation\"].remove_columns(['sentence1', 'sentence2', 'idx'])\ntest_dataset = tokenized_datasets[\"test\"].remove_columns(['sentence1', 'sentence2', 'idx'])\n\n# Regression Metric Function for Trainer\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    # Squeeze predictions to remove single-dimensional entries (e.g., shape (N, 1) to (N,))\n    predictions = predictions.squeeze() \n    \n    pearson, _ = pearsonr(predictions, labels)\n    spearman, _ = spearmanr(predictions, labels)\n    \n    # **ACTION REQUIRED: Confusion Matrix**\n    # To compute a Confusion Matrix, you'd need to quantize the continuous scores (0-5) into discrete classes.\n    # Example: score_to_class(score) function, then calculate CM.\n    # cm = confusion_matrix(labels_quantized, predictions_quantized)\n    \n    return {\"pearson\": pearson, \"spearman\": spearman}\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### 2.2. Model Training and Evaluation (Model 1: BERT)\n\nmodel_1 = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME_1, num_labels=1) \n\ntraining_args_1 = TrainingArguments(\n    output_dir=\"./results_bert\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model='pearson',\n)\n\ntrainer_1 = Trainer(\n    model=model_1,\n    args=training_args_1,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset,\n    compute_metrics=compute_metrics,\n)\n\n# **ACTION REQUIRED: Training**\n# trainer_1.train()\n\n# **ACTION REQUIRED: Evaluation**\n# results_1 = trainer_1.evaluate(test_dataset)\n# print(f\"\\n--- Model 1 ({MODEL_NAME_1}) Test Results ---\")\n# print(results_1)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### 2.3. Model Training and Evaluation (Model 2: RoBERTa)\n\n# **ACTION REQUIRED: Repeat the process for RoBERTa**\n# model_2 = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME_2, num_labels=1)\n# ... define training_args_2, trainer_2 ...\n# trainer_2.train()\n# results_2 = trainer_2.evaluate(test_dataset)\n# print(f\"\\n--- Model 2 ({MODEL_NAME_2}) Test Results ---\")\n# print(results_2)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Custom RAG (Retrieval-Augmented Generation) Mechanism\n\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### 3.1. Knowledge Base Preparation\n\n# **ACTION REQUIRED: Define or Load Knowledge Base**\nKB_DOCS = [\n    \"Doc 1: Transformer architecture uses multi-head attention to weigh the importance of different words in the input sequence. This allows parallelization.\",\n    \"Doc 2: Recurrent Neural Networks (RNNs) suffer from the vanishing gradient problem, which LSTMs and GRUs were designed to solve through gating mechanisms.\",\n    # Add your documents here\n]\n\n# **ACTION REQUIRED: Chunking and Embedding**\n# 1. Chunk documents (e.g., using a text splitter).\n# 2. Embed the chunks (e.g., using 'sentence-transformers/all-MiniLM-L6-v2').\n# 3. Store chunks and embeddings in a Vector Store (e.g., simple list, Faiss, or Chroma).\n\n# Example Placeholder for a simple in-memory store (requires implementation of actual embedding)\n# class SimpleVectorStore:\n#     def __init__(self, docs, embed_model):\n#         self.chunks = docs\n#         self.embeddings = [embed_model.encode(c) for c in docs]\n#         self.embed_model = embed_model\n#     def search(self, query, k=3):\n#         query_embedding = self.embed_model.encode(query)\n#         # Implement similarity calculation (e.g., cosine similarity) and return top k chunks\n#         return self.chunks[:k] # Placeholder return\n# embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n# vector_store = SimpleVectorStore(KB_DOCS, embed_model)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### 3.2. Retrieval and Generation Logic\n\n# **ACTION REQUIRED: Define your LLM for generation**\n# llm_pipeline = pipeline('text-generation', model='your-small-llm-model')\n\ndef custom_rag_mechanism(query, top_k=3):\n    \"\"\"Performs retrieval and augmented generation.\"\"\"\n    \n    # --- Step 1: Retrieval (Requires `vector_store` from above) ---\n    # retrieved_chunks = vector_store.search(query, k=top_k)\n    retrieved_chunks = [\"<retrieved_chunk_1>\", \"<retrieved_chunk_2>\"] # TEMPORARY\n    \n    # --- Step 2: Context Formatting ---\n    context = \"\\n\\n\".join(retrieved_chunks)\n    \n    # --- Step 3: Generation (LLM Call) ---\n    LLM_PROMPT = f\"\"\"\n    You are an AI assistant. Use the following CONTEXT to answer the USER QUESTION. \n    If the CONTEXT does not contain the answer, state that you cannot answer based on the provided information.\n\n    CONTEXT:\n    {context}\n\n    USER QUESTION: {query}\n\n    ANSWER:\n    \"\"\"\n    \n    # response = llm_pipeline(LLM_PROMPT, max_new_tokens=100)[0]['generated_text']\n    response = \"RAG Answer based on the retrieved context!\"\n    return response, retrieved_chunks\n\n# Example Test\nuser_query = \"What were LSTMs designed to solve in traditional RNNs?\"\nanswer, context_used = custom_rag_mechanism(user_query)\nprint(f\"Query: {user_query}\")\nprint(f\"Answer: {answer}\")\nprint(f\"Context Used: {context_used}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. LLM Tuning with DPO for Gordon Ramsay Persona\n\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### 5.1. Dataset Preparation\n\n# **ACTION REQUIRED: Update file path**\nPREFERENCE_DATA_PATH = \"path/to/task4_ramsay_preference_dataset.csv\" \n\n# Load the dataset created in Task 4\n# DPO_DF = pd.read_csv(PREFERENCE_DATA_PATH)\n\n# DPO requires (prompt, chosen_response, rejected_response)\n# dpo_dataset_df = DPO_DF.rename(columns={\n#     'Question': 'prompt',\n#     'Ramsay': 'chosen', # This is the preferred answer\n#     'Polite': 'rejected' # This is the non-preferred answer\n# })\n\n# Convert to Hugging Face Dataset format (requires `from datasets import Dataset`)\n# dpo_hf_dataset = Dataset.from_pandas(dpo_dataset_df[['prompt', 'chosen', 'rejected']])\n# dpo_hf_dataset = dpo_hf_dataset.train_test_split(test_size=0.1) # Split for validation\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### 5.2. Model and DPO Trainer Setup\n\n# **ACTION REQUIRED: Choose your base model**\nBASE_MODEL = \"facebook/opt-125m\" # Use a small model for development, or a larger one if resources allow\n\n# 1. Load the base model and tokenizer\n# model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, torch_dtype=torch.bfloat16) # Use a suitable dtype\n# tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n# tokenizer.pad_token = tokenizer.eos_token # Ensure pad token is set\n\n# 2. Setup PEFT/LoRA configuration\n# peft_config = LoraConfig(\n#     r=16,\n#     lora_alpha=16,\n#     target_modules=[\"q_proj\", \"v_proj\"], # Check model documentation for correct layers\n#     lora_dropout=0.05,\n#     bias=\"none\",\n#     task_type=\"CAUSAL_LM\",\n# )\n\n# 3. DPO Training Arguments\n# training_args_dpo = TrainingArguments(\n#     output_dir=\"./dpo_results_ramsay\",\n#     num_train_epochs=1,\n#     per_device_train_batch_size=4,\n#     gradient_accumulation_steps=4,\n#     logging_steps=10,\n#     learning_rate=5e-5,\n#     remove_unused_columns=False,\n#     save_strategy=\"epoch\",\n#     fp16=True, # Use fp16/bf16 if supported\n#     report_to=\"none\"\n# )\n\n# 4. Initialize DPOTrainer\n# dpo_trainer = DPOTrainer(\n#     model=model,\n#     ref_model=None, # Set to None for implicit reference model loading\n#     args=training_args_dpo,\n#     beta=0.1, \n#     train_dataset=dpo_hf_dataset['train'],\n#     eval_dataset=dpo_hf_dataset['test'],\n#     tokenizer=tokenizer,\n#     peft_config=peft_config,\n# )\n\n# 5. Train Placeholder\n# print(\"Starting DPO Training...\")\n# dpo_trainer.train()\n\n# 6. Save the final model (LoRA weights)\n# dpo_trainer.save_model(\"ramsay_dpo_adapter\")\n\n# **ACTION REQUIRED: Inference Test**\n# Test the fine-tuned model with a new question to verify the Ramsay persona.\n# from peft import PeftModel\n# ft_model = PeftModel.from_pretrained(model, \"ramsay_dpo_adapter\")\n# ft_model.eval()\n# print(\"\\n--- DPO Fine-Tuned Model Test ---\")\n# test_prompt = \"Why is batch normalization useful?\"\n# ... generate response ...\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}